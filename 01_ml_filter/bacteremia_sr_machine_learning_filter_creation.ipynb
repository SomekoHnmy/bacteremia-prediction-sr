{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning–Assisted Screening Workflow\n",
    "\n",
    "Following manual screening of 500 records, a supervised machine learning classifier was trained using LightGBM. The process included:\n",
    "\n",
    "1. Preprocessing the title and abstract text using TF-IDF vectorization.\n",
    "2. Training a binary classifier to distinguish eligible from ineligible studies, based on manually labeled training data.\n",
    "3. Hyperparameter tuning with Optuna to optimize model performance, with emphasis on high recall.\n",
    "4. Applying the trained model to the remaining records to rank them by predicted relevance.\n",
    "5. Exporting prediction scores for further manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install necessary library\n",
    "pip install bibtexparser\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Confirm current directory\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the new directory path\n",
    "new_directory = os.path.join(current_directory, 'Python', 'Torayyan')\n",
    "\n",
    "# Change the current working directory to the new directory\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm current directory\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#中断して再開する場合はcsvにしておかないかぎり毎回ここからになるよう\n",
    "with open('My Collection.bib', encoding = 'utf-8') as bibtex_file:\n",
    "    bib_database = bibtexparser.load(bibtex_file)\n",
    "\n",
    "# bib_database.entries でエントリ一覧が取得できる\n",
    "print(\"BibTeXに含まれるエントリ数:\")\n",
    "print(len(bib_database.entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import bibtexparser\n",
    "\n",
    "# 1. BibTeXデータベースを保存する\n",
    "def save_bibdatabase(bib_database, filename='bibdatabase.pickle'):\n",
    "    \"\"\"\n",
    "    BibTeXデータベースをpickleファイルとして保存\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(bib_database, f)\n",
    "    print(f\"BibTeXデータベースを {filename} に保存しました\")\n",
    "\n",
    "# 2. 保存したBibTeXデータベースを読み込む\n",
    "def load_bibdatabase(filename='bibdatabase.pickle'):\n",
    "    \"\"\"\n",
    "    保存したBibTeXデータベースを読み込む\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            bib_database = pickle.load(f)\n",
    "        print(f\"{filename} からBibTeXデータベースを読み込みました\")\n",
    "        print(f\"エントリ数: {len(bib_database.entries)}\")\n",
    "        return bib_database\n",
    "    else:\n",
    "        print(f\"保存されたファイル {filename} が見つかりません\")\n",
    "        return None\n",
    "\n",
    "# 使い方の例\n",
    "\n",
    "# 1. BibTeXファイルを最初に読み込んだ後、保存する\n",
    "#with open('My Collection.bib', encoding='utf-8') as bibtex_file:\n",
    "#    bib_database = bibtexparser.load(bibtex_file)\n",
    "#print(f\"BibTeXに含まれるエントリ数: {len(bib_database.entries)}\")\n",
    "\n",
    "# データベースを保存\n",
    "save_bibdatabase(bib_database)\n",
    "\n",
    "# 2. 次回のセッションでは、pickleファイルから直接読み込む\n",
    "# bib_database = load_bibdatabase()\n",
    "\n",
    "# 必要に応じて再度BibTeXファイルから読み込む場合\n",
    "# with open('My Collection.bib', encoding='utf-8') as bibtex_file:\n",
    "#     bib_database = bibtexparser.load(bibtex_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# ランダムに50件を抽出\n",
    "random_entries = random.sample(bib_database.entries, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_entry(entry):\n",
    "    # 各フィールドを取得（存在しない場合は空文字を返す）\n",
    "    title = entry.get('title', '').strip('{}')\n",
    "    \n",
    "    # 著者の整形（複数著者は and で区切られている）\n",
    "    authors = entry.get('author', '').replace('\\n', ' ')\n",
    "    authors = authors.strip('{}')\n",
    "    \n",
    "    # 掲載誌/会議名\n",
    "    journal = entry.get('journal', entry.get('booktitle', ''))\n",
    "    \n",
    "    # 出版年\n",
    "    year = entry.get('year', '')\n",
    "    \n",
    "    # DOI\n",
    "    doi = entry.get('doi', '')\n",
    "    \n",
    "    # Abstract\n",
    "    abstract = entry.get('abstract', '').strip('{}')\n",
    "    \n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Authors': authors,\n",
    "        'Abstract': abstract,\n",
    "        'Journal': journal,\n",
    "        'Year': year,\n",
    "        'DOI': doi\n",
    "    }\n",
    "\n",
    "# 全エントリを変換\n",
    "rayyan_entries = [format_entry(entry) for entry in random_entries]\n",
    "\n",
    "# DataFrameに変換\n",
    "df = pd.DataFrame(rayyan_entries)\n",
    "\n",
    "# CSVファイルとして保存\n",
    "output_file = 'rayyan_import.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"50件のエントリを {output_file} に保存しました。\")\n",
    "\n",
    "# 最初の数件を表示して確認\n",
    "print(\"\\n最初の3件のエントリ:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
    "# エントリのハッシュを計算する関数\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "def get_entry_hash(entry):\n",
    "    entry_str = json.dumps(entry, sort_keys=True)\n",
    "    return hashlib.md5(entry_str.encode()).hexdigest()\n",
    "\n",
    "# 既に抽出したエントリのハッシュを取得\n",
    "# random_entriesとrandom_entries_500の両方\n",
    "already_extracted_hashes = set()\n",
    "\n",
    "# 最初の50件のハッシュを追加\n",
    "try:\n",
    "    for entry in random_entries:\n",
    "        already_extracted_hashes.add(get_entry_hash(entry))\n",
    "    print(f\"50件のエントリのハッシュを追加しました\")\n",
    "except NameError:\n",
    "    print(\"random_entriesが見つかりません\")\n",
    "\n",
    "# 残りのエントリを取得\n",
    "remaining_entries = []\n",
    "for entry in bib_database.entries:\n",
    "    entry_hash = get_entry_hash(entry)\n",
    "    if entry_hash not in already_extracted_hashes:\n",
    "        remaining_entries.append(entry)\n",
    "\n",
    "print(f\"残りのエントリ数: {len(remaining_entries)}\")\n",
    "\n",
    "# 残りのエントリから500件をランダムに抽出 \n",
    "sample_size = min(500, len(remaining_entries))\n",
    "random_entries_500 = random.sample(remaining_entries, sample_size)\n",
    "\n",
    "# 次の500件のハッシュを追加\n",
    "try:\n",
    "    for entry in random_entries_500:\n",
    "        already_extracted_hashes.add(get_entry_hash(entry))\n",
    "    print(f\"500件のエントリのハッシュを追加しました\")\n",
    "except NameError:\n",
    "    print(\"random_entries_500が見つかりません\")\n",
    "\n",
    "# 抽出済みのハッシュの数を確認\n",
    "print(f\"抽出済みのエントリ数: {len(already_extracted_hashes)}\")\n",
    "\n",
    "# Rayyan用のデータを整形する関数（PMIDの検索機能を含む）\n",
    "def format_entry(entry):\n",
    "    # 各フィールドを取得（存在しない場合は空文字を返す）\n",
    "    title = entry.get('title', '').strip('{}')\n",
    "    \n",
    "    # 著者の整形（複数著者は and で区切られている）\n",
    "    authors = entry.get('author', '').replace('\\n', ' ')\n",
    "    authors = authors.strip('{}')\n",
    "    \n",
    "    # 掲載誌/会議名\n",
    "    journal = entry.get('journal', entry.get('booktitle', ''))\n",
    "    \n",
    "    # 出版年\n",
    "    year = entry.get('year', '')\n",
    "    \n",
    "    # DOI\n",
    "    doi = entry.get('doi', '')\n",
    "    \n",
    "    # Abstract\n",
    "    abstract = entry.get('abstract', '').strip('{}')\n",
    "    \n",
    "    # PMID - まず直接pmidフィールドを探し、なければnoteフィールド内を検索\n",
    "    pmid = entry.get('pmid', '')\n",
    "    if not pmid and 'note' in entry:\n",
    "        note = entry['note']\n",
    "        # noteフィールド内でPMIDを探す (例: \"PMID: 12345678\" または \"pmid=12345678\" など)\n",
    "        pmid_match = re.search(r'(?:PMID|pmid)[:\\s=]+(\\d+)', note)\n",
    "        if pmid_match:\n",
    "            pmid = pmid_match.group(1)\n",
    "    \n",
    "    return {\n",
    "        'Title': title,\n",
    "        'Authors': authors,\n",
    "        'Abstract': abstract,\n",
    "        'Journal': journal,\n",
    "        'Year': year,\n",
    "        'DOI': doi,\n",
    "        'PMID': pmid\n",
    "    }\n",
    "# 500件サンプルをRayyan用CSVに書き出し\n",
    "\n",
    "\n",
    "# 残りの全エントリを変換（時間がかかる可能性あり）\n",
    "print(\"残りのエントリをDataFrameに変換中...\")\n",
    "all_remaining_entries = [format_entry(entry) for entry in remaining_entries]\n",
    "\n",
    "# DataFrameに変換\n",
    "remaining_df = pd.DataFrame(all_remaining_entries)\n",
    "\n",
    "# 基本的な統計情報を表示\n",
    "print(\"\\nDataFrameの基本情報:\")\n",
    "print(remaining_df.info())\n",
    "\n",
    "print(\"\\n欠損値の数:\")\n",
    "print(remaining_df.isnull().sum())\n",
    "\n",
    "print(\"\\n最初の3件のエントリ:\")\n",
    "print(remaining_df.head(3))\n",
    "\n",
    "# CSV保存オプション（大きいファイルになる可能性があるのでコメントアウト）\n",
    "# remaining_df.to_csv('remaining_entries.csv', index=False, encoding='utf-8')\n",
    "# print(\"残りのエントリをremaining_entries.csvに保存しました\")\n",
    "\n",
    "# タイトルと抄録を組み合わせた新しい列'tiab'を作成（機械学習用）\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['Title']),\n",
    "        'abstract': str(row['Abstract'])\n",
    "    })\n",
    "\n",
    "print(\"機械学習用のtiab列を生成中...\")\n",
    "remaining_df['tiab'] = remaining_df.apply(create_json_text, axis=1)\n",
    "\n",
    "print(\"変換完了！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bibtexparser\n",
    "import hashlib\n",
    "import json\n",
    "import re\n",
    "\n",
    "# CSVファイルを読み込む\n",
    "print(\"CSVファイルを読み込み中...\")\n",
    "df_50 = pd.read_csv('rayyan_import_trial50.csv')\n",
    "df_500 = pd.read_csv('rayyan_import_1stml_500.csv')\n",
    "\n",
    "print(f\"50件のCSVには {len(df_50)} 行あります\")\n",
    "print(f\"500件のCSVには {len(df_500)} 行あります\")\n",
    "\n",
    "# BibTeXファイルを読み込む（まだ読み込んでいない場合）\n",
    "try:\n",
    "    print(f\"既存のBibTeXデータベース（{len(bib_database.entries)}件）を使用します\")\n",
    "except NameError:\n",
    "    print(\"BibTeXファイルを読み込み中...\")\n",
    "    with open('My Collection.bib', encoding='utf-8') as bibtex_file:\n",
    "        bib_database = bibtexparser.load(bibtex_file)\n",
    "    print(f\"BibTeXデータベースを読み込みました（{len(bib_database.entries)}件）\")\n",
    "\n",
    "# エントリのハッシュを計算する関数\n",
    "def get_entry_hash(entry):\n",
    "    entry_str = json.dumps(entry, sort_keys=True)\n",
    "    return hashlib.md5(entry_str.encode()).hexdigest()\n",
    "\n",
    "# タイトルと著者でBibTeXエントリを検索する関数\n",
    "def find_bibtex_entry(title, authors, abstract=None):\n",
    "    matches = []\n",
    "    \n",
    "    for entry in bib_database.entries:\n",
    "        entry_title = entry.get('title', '').strip('{}').lower()\n",
    "        entry_authors = entry.get('author', '').replace('\\n', ' ').strip('{}').lower()\n",
    "        \n",
    "        # タイトルが一致するかチェック\n",
    "        if title.lower() in entry_title or entry_title in title.lower():\n",
    "            # 著者が部分一致するかチェック\n",
    "            if authors.lower() in entry_authors or entry_authors in authors.lower():\n",
    "                matches.append(entry)\n",
    "    \n",
    "    # 一致するものが複数ある場合はアブストラクトで絞り込む\n",
    "    if len(matches) > 1 and abstract:\n",
    "        abstract = str(abstract).lower()\n",
    "        for entry in matches:\n",
    "            entry_abstract = entry.get('abstract', '').strip('{}').lower()\n",
    "            if abstract in entry_abstract or entry_abstract in abstract:\n",
    "                return entry\n",
    "    \n",
    "    # 一致するものがある場合は最初のものを返す\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# CSVからBibTeXエントリを復元\n",
    "def restore_entries_from_csv(df):\n",
    "    restored_entries = []\n",
    "    not_found = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        title = str(row['Title'])\n",
    "        authors = str(row['Authors'])\n",
    "        abstract = str(row['Abstract']) if 'Abstract' in df.columns else None\n",
    "        \n",
    "        entry = find_bibtex_entry(title, authors, abstract)\n",
    "        if entry:\n",
    "            restored_entries.append(entry)\n",
    "        else:\n",
    "            not_found += 1\n",
    "            print(f\"警告: エントリが見つかりません: {title}\")\n",
    "    \n",
    "    print(f\"復元されたエントリ: {len(restored_entries)}, 見つからなかったエントリ: {not_found}\")\n",
    "    return restored_entries\n",
    "\n",
    "# CSVから50件と500件のエントリを復元\n",
    "print(\"\\n50件のエントリを復元中...\")\n",
    "random_entries = restore_entries_from_csv(df_50)\n",
    "\n",
    "print(\"\\n500件のエントリを復元中...\")\n",
    "random_entries_500 = restore_entries_from_csv(df_500)\n",
    "\n",
    "# 復元されたエントリのハッシュを作成\n",
    "already_extracted_hashes = set()\n",
    "for entry in random_entries:\n",
    "    already_extracted_hashes.add(get_entry_hash(entry))\n",
    "for entry in random_entries_500:\n",
    "    already_extracted_hashes.add(get_entry_hash(entry))\n",
    "\n",
    "print(f\"\\n復元された抽出済みエントリの合計: {len(already_extracted_hashes)}\")\n",
    "\n",
    "# 残りのエントリを取得\n",
    "remaining_entries = []\n",
    "for entry in bib_database.entries:\n",
    "    entry_hash = get_entry_hash(entry)\n",
    "    if entry_hash not in already_extracted_hashes:\n",
    "        remaining_entries.append(entry)\n",
    "\n",
    "print(f\"残りのエントリ数: {len(remaining_entries)}\")\n",
    "\n",
    "# 結果確認用に最初のいくつかのエントリを表示\n",
    "if random_entries:\n",
    "    print(\"\\n復元された最初のエントリのタイトル (50件):\")\n",
    "    print(random_entries[0].get('title', '').strip('{}'))\n",
    "\n",
    "if random_entries_500:\n",
    "    print(\"\\n復元された最初のエントリのタイトル (500件):\")\n",
    "    print(random_entries_500[0].get('title', '').strip('{}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "from typing import TypeVar, Type, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "rayyan_decided_df = pd.read_csv('rayyan_decided_1st500.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(rayyan_decided_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rayyan_decided_dfに対してincludedフラグを作成\n",
    "# 'notes'列に'Included'という文字列が含まれていればTrue\n",
    "rayyan_decided_df['included'] = rayyan_decided_df['notes'].astype(str).str.contains('Included')\n",
    "\n",
    "# 結果の集計を表示\n",
    "print(rayyan_decided_df['included'].value_counts())\n",
    "\n",
    "# 不整合（'included'がTrueなのに'notes'に'Excluded'が含まれる）の件数\n",
    "inconsistency_count = rayyan_decided_df[rayyan_decided_df[\"included\"] == True][\"notes\"].str.contains(\"Excluded\").sum()\n",
    "print(f'不整合の件数: {inconsistency_count}')\n",
    "\n",
    "# タイトルと抄録を組み合わせた新しい列'tiab'を作成\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['title']),\n",
    "        'abstract': str(row['abstract'])\n",
    "    })\n",
    "\n",
    "# 'tiab'列を作成\n",
    "rayyan_decided_df['tiab'] = rayyan_decided_df.apply(create_json_text, axis=1)\n",
    "\n",
    "# 最初の数行を表示して確認\n",
    "print(\"\\n最初の3行のデータ:\")\n",
    "print(rayyan_decided_df[['title', 'abstract', 'notes', 'included', 'tiab']].head(3))\n",
    "\n",
    "# 必要に応じてCSVとして保存\n",
    "# rayyan_decided_df.to_csv('rayyan_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_data(df, text_column='tiab', label_column='included'):\n",
    "    \"\"\"テキストデータの前処理とTF-IDF変換\"\"\"\n",
    "    texts = df[text_column].fillna('')\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "\n",
    "    X_vec = vectorizer.fit_transform(texts)\n",
    "    y = df[label_column]\n",
    "\n",
    "    class_weights = dict(zip(\n",
    "        y.unique(),\n",
    "        [1 / (len(y) * (y == label).mean()) for label in y.unique()]\n",
    "    ))\n",
    "\n",
    "    return X_vec, y, vectorizer, class_weights\n",
    "\n",
    "def fbeta_score_custom(y_true, y_pred, beta=1):\n",
    "    \"\"\"カスタムF-betaスコアの計算\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    if precision == 0.0 and recall == 0.0:\n",
    "        return 0.0\n",
    "    fbeta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    return fbeta\n",
    "\n",
    "def calculate_detailed_metrics(y_true, y_pred):\n",
    "    \"\"\"詳細な評価指標の計算と表示\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "\n",
    "    print(\"\\n=== 詳細な評価指標 ===\")\n",
    "    print(f\"感度 (Sensitivity/Recall): {sensitivity:.3f}\")\n",
    "    print(f\"特異度 (Specificity): {specificity:.3f}\")\n",
    "    print(f\"適合率 (Precision): {precision:.3f}\")\n",
    "    print(f\"F1スコア: {f1:.3f}\")\n",
    "\n",
    "    print(\"\\n=== 混同行列 ===\")\n",
    "    print(\"                  Predicted\")\n",
    "    print(\"                  Negative  Positive\")\n",
    "    print(f\"Actual Negative    {tn:^8} {fp:^8}\")\n",
    "    print(f\"      Positive    {fn:^8} {tp:^8}\")\n",
    "\n",
    "    return {\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'precision': precision,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}\n",
    "    }\n",
    "\n",
    "def evaluate_model(X_vec, y, params, threshold, class_weights, beta=2):\n",
    "    \"\"\"モデルの評価（交差検証）\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    all_predictions = []\n",
    "    all_true_values = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_vec, y):\n",
    "        X_tr = X_vec[train_idx]\n",
    "        X_val = X_vec[val_idx]\n",
    "        y_tr = y.iloc[train_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        model = LGBMClassifier(\n",
    "            **params,\n",
    "            random_state=42,\n",
    "            class_weight=class_weights\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_prob = model.predict_proba(X_val)[:, 1]\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "\n",
    "        score = fbeta_score_custom(y_val, y_pred, beta=beta)\n",
    "        scores.append(score)\n",
    "\n",
    "        all_predictions.extend(y_pred)\n",
    "        all_true_values.extend(y_val)\n",
    "\n",
    "    if len(scores) == 5:\n",
    "        print(\"\\n=== 交差検証全体の評価 ===\")\n",
    "        metrics = calculate_detailed_metrics(all_true_values, all_predictions)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_true_values, all_predictions))\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def objective(trial, X_vec, y, class_weights, beta=2):\n",
    "    \"\"\"Optunaの目的関数\"\"\"\n",
    "    params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 128),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"verbosity\": -1\n",
    "    }\n",
    "    threshold = trial.suggest_float(\"threshold\", 0.1, 0.9)\n",
    "\n",
    "    score = evaluate_model(X_vec, y, params, threshold, class_weights, beta=beta)\n",
    "    return score\n",
    "\n",
    "def train_final_model(df, beta_value=2, n_trials=50):\n",
    "    \"\"\"完全なトレーニングパイプライン\"\"\"\n",
    "    # データの準備\n",
    "    X_vec, y, vectorizer, class_weights = prepare_text_data(df)\n",
    "\n",
    "    print(f\"データセットの形状: {X_vec.shape}\")\n",
    "    print(f\"クラスの分布:\\n{y.value_counts(normalize=True)}\")\n",
    "    print(f\"クラスの重み: {class_weights}\")\n",
    "\n",
    "    # ハイパーパラメータの最適化\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_vec, y, class_weights, beta=beta_value),\n",
    "        n_trials=n_trials\n",
    "    )\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    threshold = best_params.pop(\"threshold\")\n",
    "\n",
    "    print(\"\\nBest parameters:\", best_params)\n",
    "    print(\"Best threshold:\", threshold)\n",
    "\n",
    "    final_score = evaluate_model(X_vec, y, best_params, threshold, class_weights, beta=beta_value)\n",
    "    print(f\"\\nFinal Mean F{beta_value} Score:\", final_score)\n",
    "\n",
    "    # 最終モデルの学習\n",
    "    final_model = LGBMClassifier(\n",
    "        **best_params,\n",
    "        random_state=42,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    final_model.fit(X_vec, y)\n",
    "\n",
    "    # 最終評価\n",
    "    print(\"\\n=== 最終モデルの評価（全データ） ===\")\n",
    "    y_prob = final_model.predict_proba(X_vec)[:, 1]\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    final_metrics = calculate_detailed_metrics(y, y_pred)\n",
    "\n",
    "    return final_model, vectorizer, threshold, best_params\n",
    "\n",
    "def predict_new_text(text, model, vectorizer, threshold):\n",
    "    \"\"\"新しいテキストの予測\"\"\"\n",
    "    X_new = vectorizer.transform([text])\n",
    "    prob = model.predict_proba(X_new)[0, 1]\n",
    "    prediction = prob >= threshold\n",
    "    return prediction, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    # データの読み込み\n",
    "    #df = pd.read_csv('your_data.csv')  # tiab列とincluded列を含むデータ\n",
    "\n",
    "    # モデルのトレーニング\n",
    "    model, vectorizer, threshold, best_params = train_final_model(\n",
    "        rayyan_decided_df,\n",
    "        beta_value=4,  # より高い値で再現率を重視\n",
    "        n_trials=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 日時をstringsで\n",
    "timestamp = time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# モデルと関連オブジェクトの保存\n",
    "model_data = {\n",
    "    'model': model,\n",
    "    'vectorizer': vectorizer,\n",
    "    'threshold': threshold,\n",
    "    'best_params': best_params\n",
    "}\n",
    "joblib.dump(model_data, f'{timestamp}_v2_model_data.joblib')\n",
    "print(\"\\nモデルと関連オブジェクトを保存しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = glob.glob('*.joblib')\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存したモデルデータをロード\n",
    "model_data = joblib.load(models[2])\n",
    "\n",
    "# 各オブジェクトの取り出し\n",
    "loaded_model = model_data['model']\n",
    "loaded_vectorizer = model_data['vectorizer']\n",
    "loaded_threshold = model_data['threshold']\n",
    "loaded_best_params = model_data['best_params']\n",
    "\n",
    "print(\"モデルと関連オブジェクトをロードしました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import joblib\n",
    "\n",
    "# モデルファイルを検索\n",
    "models = glob.glob('*.joblib')\n",
    "print(f\"見つかったモデルファイル: {models}\")\n",
    "\n",
    "if not models:\n",
    "    print(\"joblib ファイルが見つかりません。\")\n",
    "else:\n",
    "    # インデックスを修正 - 最初の要素を使用\n",
    "    model_file = models[0]\n",
    "    print(f\"使用するモデルファイル: {model_file}\")\n",
    "    \n",
    "    try:\n",
    "        # 保存したモデルデータをロード\n",
    "        model_data = joblib.load(model_file)\n",
    "        \n",
    "        # 各オブジェクトの取り出し\n",
    "        loaded_model = model_data['model']\n",
    "        loaded_vectorizer = model_data['vectorizer']\n",
    "        loaded_threshold = model_data['threshold']\n",
    "        loaded_best_params = model_data['best_params']\n",
    "        \n",
    "        print(\"モデルと関連オブジェクトをロードしました。\")\n",
    "        print(f\"モデルの種類: {type(loaded_model).__name__}\")\n",
    "        print(f\"最適な閾値: {loaded_threshold}\")\n",
    "        print(f\"最適なパラメータ: {loaded_best_params}\")\n",
    "    except Exception as e:\n",
    "        print(f\"モデルのロード中にエラーが発生しました: {str(e)}\")\n",
    "        \n",
    "        # ファイルの内容を確認\n",
    "        try:\n",
    "            test_load = joblib.load(model_file)\n",
    "            print(f\"ファイルには次のキーが含まれています: {list(test_load.keys()) if isinstance(test_load, dict) else '辞書ではありません'}\")\n",
    "        except:\n",
    "            print(\"ファイルの内容を確認できませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_summarize(df, create_json_text, loaded_vectorizer, loaded_model, loaded_threshold):\n",
    "    # \"tiab\"列の作成 (DataFrame dfに対して適用)\n",
    "    df[\"tiab\"] = df.apply(create_json_text, axis=1)\n",
    "\n",
    "    # テキストデータの前処理とベクトル化\n",
    "    X_new = loaded_vectorizer.transform(df[\"tiab\"].fillna(''))\n",
    "\n",
    "    print(f\"新しいデータの形状: {X_new.shape}\")\n",
    "\n",
    "    # 予測確率の取得\n",
    "    y_prob_new = loaded_model.predict_proba(X_new)[:, 1]\n",
    "\n",
    "    # 閾値を適用してクラスラベルを決定\n",
    "    y_pred_new = (y_prob_new >= loaded_threshold).astype(int)\n",
    "\n",
    "    # 予測結果をデータフレームに追加\n",
    "    df['prediction'] = y_pred_new\n",
    "    df['probability'] = y_prob_new\n",
    "\n",
    "    print(\"予測を実行し、結果をデータフレームに追加しました。\")\n",
    "\n",
    "    # 予測結果の表示\n",
    "    print(\"\\n=== 予測結果 ===\")\n",
    "    print(df[['tiab', 'prediction', 'probability']])\n",
    "\n",
    "    # クラス分布の確認\n",
    "    class_dist = df['prediction'].value_counts()\n",
    "    print(\"\\nクラスの分布:\")\n",
    "    print(class_dist)\n",
    "\n",
    "    # 確率の統計情報\n",
    "    prob_stats = df['probability'].describe()\n",
    "    print(\"\\n予測確率の統計情報:\")\n",
    "    print(prob_stats)\n",
    "\n",
    "    # 必要に応じて、更新後のdfや統計情報を返す\n",
    "    return df, class_dist, prob_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BibTeXエントリをDataFrameに変換する関数\n",
    "def convert_entries_to_df(entries):\n",
    "    # 各エントリからフィールドを抽出\n",
    "    formatted_entries = []\n",
    "    for entry in entries:\n",
    "        # 各フィールドを取得（存在しない場合は空文字を返す）\n",
    "        title = entry.get('title', '').strip('{}')\n",
    "        \n",
    "        # 著者の整形\n",
    "        authors = entry.get('author', '').replace('\\n', ' ').strip('{}')\n",
    "        \n",
    "        # 掲載誌/会議名\n",
    "        journal = entry.get('journal', entry.get('booktitle', ''))\n",
    "        \n",
    "        # 出版年\n",
    "        year = entry.get('year', '')\n",
    "        \n",
    "        # DOI\n",
    "        doi = entry.get('doi', '')\n",
    "        \n",
    "        # Abstract\n",
    "        abstract = entry.get('abstract', '').strip('{}')\n",
    "        \n",
    "        # PMID\n",
    "        pmid = entry.get('pmid', '')\n",
    "        \n",
    "        formatted_entries.append({\n",
    "            'Title': title,\n",
    "            'Authors': authors,\n",
    "            'Abstract': abstract,\n",
    "            'Journal': journal,\n",
    "            'Year': year,\n",
    "            'DOI': doi,\n",
    "            'PMID': pmid\n",
    "        })\n",
    "    \n",
    "    # DataFrameに変換\n",
    "    return pd.DataFrame(formatted_entries)\n",
    "\n",
    "# remaining_entriesをDataFrameに変換\n",
    "print(f\"残りの{len(remaining_entries)}件のエントリをDataFrameに変換中...\")\n",
    "remaining_df = convert_entries_to_df(remaining_entries)\n",
    "\n",
    "# tiab列を作る関数（既に定義済み）\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['Title']),\n",
    "        'abstract': str(row['Abstract'])\n",
    "    })\n",
    "\n",
    "# 機械学習フィルターを適用\n",
    "print(\"機械学習フィルターを適用中...\")\n",
    "filtered_df, class_distribution, probability_stats = predict_and_summarize(\n",
    "    remaining_df, create_json_text, loaded_vectorizer, loaded_model, loaded_threshold\n",
    ")\n",
    "\n",
    "# 「含まれる」と予測されたエントリのみを抽出\n",
    "included_df = filtered_df[filtered_df['prediction'] == 1]\n",
    "print(f\"\\n「含まれる」と予測されたエントリ: {len(included_df)}件\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索したいPMIDのリスト（文字列から数字だけを抽出）\n",
    "pmid_list_raw = \"\"\"1888246[pmid] or 2393205[pmid] or 16586387[pmid] or 9395366[pmid] or 8263576[pmid] or 14630621[pmid] or \n",
    "15482728[pmid] or 18486413[pmid] or 19555286[pmid] or 20586653[pmid] or 20299634[pmid] or 20557905[pmid] or \n",
    "21722666[pmid] or 23808716[pmid] or 24856736[pmid] or 18486413[pmid] or 20557905[pmid] or 21083886[pmid] or \n",
    "21684227[pmid] or 22463870[pmid] or 24856736[pmid] or 26656373[pmid] or 26246024[pmid] or 28714287[pmid] or \n",
    "32291611[pmid] or 27023336[pmid] or 33740571[pmid] or 33810788[pmid] or 34992000[pmid] or 34032112[pmid] or \n",
    "34487306[pmid] or 32796184[pmid] or 31581716[pmid] or 35054269[pmid] or 32646430[pmid] or 37620774[pmid] or \n",
    "35351003[pmid] or 33658812[pmid] or 34484472[pmid] or 38396484[pmid] or 30111827[pmid] or 34428529[pmid] or \n",
    "29661634[pmid] or 37519767[pmid] or 34309570[pmid] or 33347714[pmid] or 34162572[pmid] or 37719647[pmid] or \n",
    "34998038[pmid] or 37598221[pmid] or 34983764[pmid] or 37244761[pmid] or 35853298[pmid] or 36292187[pmid] or \n",
    "37562317[pmid] or 31912208[pmid] or 30420241[pmid] or 34209759[pmid] or 29486341[pmid] or 34490306[pmid]\"\"\"\n",
    "\n",
    "# 数字のみを抽出\n",
    "pmid_list = re.findall(r'(\\d+)', pmid_list_raw)\n",
    "\n",
    "# 重複を除去（リスト内に重複PMIDがある）\n",
    "pmid_list = list(set(pmid_list))\n",
    "\n",
    "print(f\"検索するPMID数: {len(pmid_list)}\")\n",
    "\n",
    "# included_dfのPMID列を確認\n",
    "if 'PMID' not in included_df.columns:\n",
    "    print(\"警告: included_dfにPMID列が見つかりません。確認できません。\")\n",
    "else:\n",
    "    # 結果を格納する辞書\n",
    "    filtered_results = {\n",
    "        'found': [],    # フィルター結果に見つかったPMID\n",
    "        'not_found': [] # フィルター結果に見つからなかったPMID\n",
    "    }\n",
    "    \n",
    "    # PMIDの存在確認（直接マッチング）\n",
    "    for target_pmid in pmid_list:\n",
    "        # PMID列で直接一致を確認\n",
    "        mask = included_df['PMID'].astype(str) == target_pmid\n",
    "        if mask.any():\n",
    "            # 見つかった場合、その行の情報を保存\n",
    "            matching_rows = included_df[mask]\n",
    "            for _, row in matching_rows.iterrows():\n",
    "                filtered_results['found'].append({\n",
    "                    'pmid': target_pmid,\n",
    "                    'title': row['Title'],\n",
    "                    'probability': row['probability'] if 'probability' in included_df.columns else 'N/A'\n",
    "                })\n",
    "        else:\n",
    "            # 見つからなかった場合\n",
    "            filtered_results['not_found'].append(target_pmid)\n",
    "    \n",
    "    # 結果を表示\n",
    "    print(\"\\n=== 機械学習フィルター結果内のPMID検索結果 ===\")\n",
    "    print(f\"フィルター後のエントリ数: {len(included_df)}\")\n",
    "    print(f\"検索したPMID数: {len(pmid_list)}\")\n",
    "    print(f\"フィルター結果内に見つかったPMID数: {len(filtered_results['found'])}\")\n",
    "    print(f\"フィルター結果内に見つからなかったPMID数: {len(filtered_results['not_found'])}\")\n",
    "    \n",
    "    # 見つかったPMIDの詳細を表示\n",
    "    if filtered_results['found']:\n",
    "        print(\"\\n=== フィルター結果内に見つかったPMIDの詳細 ===\")\n",
    "        for i, entry in enumerate(filtered_results['found'], 1):\n",
    "            print(f\"\\n--- {i}. PMID: {entry['pmid']} ---\")\n",
    "            print(f\"タイトル: {entry['title']}\")\n",
    "            print(f\"予測確率: {entry['probability']}\")\n",
    "    \n",
    "    # 見つからなかったPMIDのリスト\n",
    "    if filtered_results['not_found']:\n",
    "        print(\"\\n=== フィルター結果内に見つからなかったPMID ===\")\n",
    "        # 10個ずつ表示\n",
    "        for i in range(0, len(filtered_results['not_found']), 10):\n",
    "            chunk = filtered_results['not_found'][i:i+10]\n",
    "            print(\", \".join(chunk))\n",
    "    \n",
    "    # 結果をファイルに保存\n",
    "    with open('filtered_pmid_results.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== フィルター結果内に見つかったPMID ===\\n\")\n",
    "        for entry in filtered_results['found']:\n",
    "            f.write(f\"PMID: {entry['pmid']}, タイトル: {entry['title']}, 予測確率: {entry['probability']}\\n\")\n",
    "        \n",
    "        f.write(\"\\n=== フィルター結果内に見つからなかったPMID ===\\n\")\n",
    "        f.write(\", \".join(filtered_results['not_found']))\n",
    "    \n",
    "    print(\"\\n検索結果をfiltered_pmid_results.txtに保存しました。\")\n",
    "\n",
    "    # PMIDが見つからなかった場合の分析\n",
    "    if filtered_results['not_found']:\n",
    "        print(\"\\n=== 見つからなかったPMIDの再検索（部分一致） ===\")\n",
    "        print(\"PMIDが部分的に含まれている可能性があるエントリを検索します...\")\n",
    "        \n",
    "        partial_matches = []\n",
    "        for target_pmid in filtered_results['not_found']:\n",
    "            # PMID列が文字列であることを確認し、部分一致を検索\n",
    "            if 'PMID' in included_df.columns:\n",
    "                mask = included_df['PMID'].astype(str).str.contains(target_pmid, na=False, regex=False)\n",
    "                matching_rows = included_df[mask]\n",
    "                \n",
    "                if not matching_rows.empty:\n",
    "                    for _, row in matching_rows.iterrows():\n",
    "                        partial_matches.append({\n",
    "                            'pmid': target_pmid,\n",
    "                            'found_in': row['PMID'],\n",
    "                            'title': row['Title']\n",
    "                        })\n",
    "        \n",
    "        if partial_matches:\n",
    "            print(f\"部分一致するPMIDが{len(partial_matches)}件見つかりました:\")\n",
    "            for i, match in enumerate(partial_matches, 1):\n",
    "                print(f\"{i}. 検索PMID: {match['pmid']}, 見つかったPMID: {match['found_in']}, タイトル: {match['title']}\")\n",
    "        else:\n",
    "            print(\"部分一致するPMIDは見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在の日時を取得（ファイル名用）\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# ステップ1: included_dfから500件をランダムに抽出\n",
    "print(f\"included_dfには{len(included_df)}件のエントリがあります\")\n",
    "\n",
    "# ランダム抽出のシード値を設定（再現性のため）\n",
    "random.seed(42)\n",
    "\n",
    "# 500件をランダムに抽出\n",
    "sample_size = min(500, len(included_df))\n",
    "sampled_indices = random.sample(range(len(included_df)), sample_size)\n",
    "sampled_df = included_df.iloc[sampled_indices].copy()\n",
    "\n",
    "# 残りのエントリを取得\n",
    "remaining_indices = [i for i in range(len(included_df)) if i not in sampled_indices]\n",
    "remaining_df = included_df.iloc[remaining_indices].copy()\n",
    "\n",
    "print(f\"ランダムに{len(sampled_df)}件抽出しました\")\n",
    "print(f\"残りのエントリ数: {len(remaining_df)}\")\n",
    "\n",
    "# ステップ2: 抽出した500件をRayyan用のCSVとして保存\n",
    "# Rayyan形式に合わせて必要なカラムを選択\n",
    "rayyan_columns = ['Title', 'Authors', 'Abstract', 'Journal', 'Year', 'DOI', 'PMID']\n",
    "rayyan_df = sampled_df[rayyan_columns].copy()\n",
    "\n",
    "# Rayyan用のCSVファイル名\n",
    "rayyan_csv_file = f\"rayyan_ml_sample_{sample_size}_{timestamp}.csv\"\n",
    "rayyan_df.to_csv(rayyan_csv_file, index=False, encoding='utf-8')\n",
    "print(f\"Rayyan用に{len(rayyan_df)}件のエントリを {rayyan_csv_file} に保存しました\")\n",
    "\n",
    "# ステップ3: 残りのエントリに対して新しい機械学習フィルターを適用する準備\n",
    "# さらにフィルタリングするためにタイトルとアブストラクトを組み合わせた列を作成\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['Title']),\n",
    "        'abstract': str(row['Abstract'])\n",
    "    })\n",
    "\n",
    "# 機械学習用のtiab列を生成\n",
    "print(\"機械学習用のtiab列を生成中...\")\n",
    "remaining_df['tiab'] = remaining_df.apply(create_json_text, axis=1)\n",
    "\n",
    "# 残りのエントリをCSVとして保存\n",
    "remaining_csv_file = f\"remaining_ml_entries_{timestamp}.csv\"\n",
    "remaining_df.to_csv(remaining_csv_file, index=False, encoding='utf-8')\n",
    "print(f\"残りの{len(remaining_df)}件のエントリを {remaining_csv_file} に保存しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd ml\n",
    "# Load the CSV file into a DataFrame\n",
    "rayyan_decided_df = pd.read_csv('rayyan_decided_2nd500.csv',encoding='cp932')\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(rayyan_decided_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rayyan_decided_dfに対してincludedフラグを作成\n",
    "# 'notes'列に'Included'という文字列が含まれていればTrue\n",
    "rayyan_decided_df['included'] = rayyan_decided_df['notes'].astype(str).str.contains('Included')\n",
    "\n",
    "# 結果の集計を表示\n",
    "print(rayyan_decided_df['included'].value_counts())\n",
    "\n",
    "# 不整合（'included'がTrueなのに'notes'に'Excluded'が含まれる）の件数\n",
    "inconsistency_count = rayyan_decided_df[rayyan_decided_df[\"included\"] == True][\"notes\"].str.contains(\"Excluded\").sum()\n",
    "print(f'不整合の件数: {inconsistency_count}')\n",
    "\n",
    "# タイトルと抄録を組み合わせた新しい列'tiab'を作成\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['title']),\n",
    "        'abstract': str(row['abstract'])\n",
    "    })\n",
    "\n",
    "# 'tiab'列を作成\n",
    "rayyan_decided_df['tiab'] = rayyan_decided_df.apply(create_json_text, axis=1)\n",
    "\n",
    "# 最初の数行を表示して確認\n",
    "print(\"\\n最初の3行のデータ:\")\n",
    "print(rayyan_decided_df[['title', 'abstract', 'notes', 'included', 'tiab']].head(3))\n",
    "\n",
    "# 必要に応じてCSVとして保存\n",
    "# rayyan_decided_df.to_csv('rayyan_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    # データの読み込み\n",
    "    #df = pd.read_csv('your_data.csv')  # tiab列とincluded列を含むデータ\n",
    "\n",
    "    # モデルのトレーニング\n",
    "    model, vectorizer, threshold, best_params = train_final_model(\n",
    "        rayyan_decided_df,\n",
    "        beta_value=4,  # より高い値で再現率を重視\n",
    "        n_trials=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日時をstringsで\n",
    "timestamp = time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# モデルと関連オブジェクトの保存\n",
    "model_data = {\n",
    "    'model': model,\n",
    "    'vectorizer': vectorizer,\n",
    "    'threshold': threshold,\n",
    "    'best_params': best_params\n",
    "}\n",
    "joblib.dump(model_data, f'{timestamp}_v2_model_data.joblib')\n",
    "print(\"\\nモデルと関連オブジェクトを保存しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = glob.glob('*.joblib')\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import joblib\n",
    "\n",
    "# モデルファイルを検索\n",
    "models = glob.glob('20250306154131_v2_model_data.joblib')\n",
    "print(f\"見つかったモデルファイル: {models}\")\n",
    "\n",
    "if not models:\n",
    "    print(\"joblib ファイルが見つかりません。\")\n",
    "else:\n",
    "    # インデックスを修正 - 最初の要素を使用\n",
    "    model_file = models[0]\n",
    "    print(f\"使用するモデルファイル: {model_file}\")\n",
    "    \n",
    "    try:\n",
    "        # 保存したモデルデータをロード\n",
    "        model_data = joblib.load(model_file)\n",
    "        \n",
    "        # 各オブジェクトの取り出し\n",
    "        loaded_model = model_data['model']\n",
    "        loaded_vectorizer = model_data['vectorizer']\n",
    "        loaded_threshold = model_data['threshold']\n",
    "        loaded_best_params = model_data['best_params']\n",
    "        \n",
    "        print(\"モデルと関連オブジェクトをロードしました。\")\n",
    "        print(f\"モデルの種類: {type(loaded_model).__name__}\")\n",
    "        print(f\"最適な閾値: {loaded_threshold}\")\n",
    "        print(f\"最適なパラメータ: {loaded_best_params}\")\n",
    "    except Exception as e:\n",
    "        print(f\"モデルのロード中にエラーが発生しました: {str(e)}\")\n",
    "        \n",
    "        # ファイルの内容を確認\n",
    "        try:\n",
    "            test_load = joblib.load(model_file)\n",
    "            print(f\"ファイルには次のキーが含まれています: {list(test_load.keys()) if isinstance(test_load, dict) else '辞書ではありません'}\")\n",
    "        except:\n",
    "            print(\"ファイルの内容を確認できませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残っている文献をcsvにしてあったので読み込む\n",
    "remaining_df = pd.read_csv('remaining_ml_entries_20250303142313.csv')\n",
    "\n",
    "# tiab列を作る関数（既に定義済み）\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['Title']),\n",
    "        'abstract': str(row['Abstract'])\n",
    "    })\n",
    "\n",
    "# 機械学習フィルターを適用\n",
    "print(\"機械学習フィルターを適用中...\")\n",
    "filtered_df, class_distribution, probability_stats = predict_and_summarize(\n",
    "    remaining_df, create_json_text, loaded_vectorizer, loaded_model, loaded_threshold\n",
    ")\n",
    "\n",
    "# 「含まれる」と予測されたエントリのみを抽出\n",
    "included_df = filtered_df[filtered_df['prediction'] == 1]\n",
    "print(f\"\\n「含まれる」と予測されたエントリ: {len(included_df)}件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索したいPMIDのリスト（文字列から数字だけを抽出）\n",
    "pmid_list_raw = \"\"\"1888246[pmid] or 2393205[pmid] or 16586387[pmid] or 9395366[pmid] or 8263576[pmid] or 14630621[pmid] or \n",
    "15482728[pmid] or 18486413[pmid] or 19555286[pmid] or 20586653[pmid] or 20299634[pmid] or 20557905[pmid] or \n",
    "21722666[pmid] or 23808716[pmid] or 24856736[pmid] or 18486413[pmid] or 20557905[pmid] or 21083886[pmid] or \n",
    "21684227[pmid] or 22463870[pmid] or 24856736[pmid] or 26656373[pmid] or 26246024[pmid] or 28714287[pmid] or \n",
    "32291611[pmid] or 27023336[pmid] or 33740571[pmid] or 33810788[pmid] or 34992000[pmid] or 34032112[pmid] or \n",
    "34487306[pmid] or 32796184[pmid] or 31581716[pmid] or 35054269[pmid] or 32646430[pmid] or 37620774[pmid] or \n",
    "35351003[pmid] or 33658812[pmid] or 34484472[pmid] or 38396484[pmid] or 30111827[pmid] or 34428529[pmid] or \n",
    "29661634[pmid] or 37519767[pmid] or 34309570[pmid] or 33347714[pmid] or 34162572[pmid] or 37719647[pmid] or \n",
    "34998038[pmid] or 37598221[pmid] or 34983764[pmid] or 37244761[pmid] or 35853298[pmid] or 36292187[pmid] or \n",
    "37562317[pmid] or 31912208[pmid] or 30420241[pmid] or 34209759[pmid] or 29486341[pmid] or 34490306[pmid]\"\"\"\n",
    "\n",
    "# 数字のみを抽出\n",
    "pmid_list = re.findall(r'(\\d+)', pmid_list_raw)\n",
    "\n",
    "# 重複を除去（リスト内に重複PMIDがある）\n",
    "pmid_list = list(set(pmid_list))\n",
    "\n",
    "print(f\"検索するPMID数: {len(pmid_list)}\")\n",
    "\n",
    "# included_dfのPMID列を確認\n",
    "if 'PMID' not in included_df.columns:\n",
    "    print(\"警告: included_dfにPMID列が見つかりません。確認できません。\")\n",
    "else:\n",
    "    # 結果を格納する辞書\n",
    "    filtered_results = {\n",
    "        'found': [],    # フィルター結果に見つかったPMID\n",
    "        'not_found': [] # フィルター結果に見つからなかったPMID\n",
    "    }\n",
    "    \n",
    "    # PMIDの存在確認（直接マッチング）\n",
    "    for target_pmid in pmid_list:\n",
    "        # PMID列で直接一致を確認\n",
    "        mask = included_df['PMID'].astype(str) == target_pmid\n",
    "        if mask.any():\n",
    "            # 見つかった場合、その行の情報を保存\n",
    "            matching_rows = included_df[mask]\n",
    "            for _, row in matching_rows.iterrows():\n",
    "                filtered_results['found'].append({\n",
    "                    'pmid': target_pmid,\n",
    "                    'title': row['Title'],\n",
    "                    'probability': row['probability'] if 'probability' in included_df.columns else 'N/A'\n",
    "                })\n",
    "        else:\n",
    "            # 見つからなかった場合\n",
    "            filtered_results['not_found'].append(target_pmid)\n",
    "    \n",
    "    # 結果を表示\n",
    "    print(\"\\n=== 機械学習フィルター結果内のPMID検索結果 ===\")\n",
    "    print(f\"フィルター後のエントリ数: {len(included_df)}\")\n",
    "    print(f\"検索したPMID数: {len(pmid_list)}\")\n",
    "    print(f\"フィルター結果内に見つかったPMID数: {len(filtered_results['found'])}\")\n",
    "    print(f\"フィルター結果内に見つからなかったPMID数: {len(filtered_results['not_found'])}\")\n",
    "    \n",
    "    # 見つかったPMIDの詳細を表示\n",
    "    if filtered_results['found']:\n",
    "        print(\"\\n=== フィルター結果内に見つかったPMIDの詳細 ===\")\n",
    "        for i, entry in enumerate(filtered_results['found'], 1):\n",
    "            print(f\"\\n--- {i}. PMID: {entry['pmid']} ---\")\n",
    "            print(f\"タイトル: {entry['title']}\")\n",
    "            print(f\"予測確率: {entry['probability']}\")\n",
    "    \n",
    "    # 見つからなかったPMIDのリスト\n",
    "    if filtered_results['not_found']:\n",
    "        print(\"\\n=== フィルター結果内に見つからなかったPMID ===\")\n",
    "        # 10個ずつ表示\n",
    "        for i in range(0, len(filtered_results['not_found']), 10):\n",
    "            chunk = filtered_results['not_found'][i:i+10]\n",
    "            print(\", \".join(chunk))\n",
    "    \n",
    "    # 結果をファイルに保存\n",
    "    with open('filtered_pmid_results.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== フィルター結果内に見つかったPMID ===\\n\")\n",
    "        for entry in filtered_results['found']:\n",
    "            f.write(f\"PMID: {entry['pmid']}, タイトル: {entry['title']}, 予測確率: {entry['probability']}\\n\")\n",
    "        \n",
    "        f.write(\"\\n=== フィルター結果内に見つからなかったPMID ===\\n\")\n",
    "        f.write(\", \".join(filtered_results['not_found']))\n",
    "    \n",
    "    print(\"\\n検索結果をfiltered_pmid_results.txtに保存しました。\")\n",
    "\n",
    "    # PMIDが見つからなかった場合の分析\n",
    "    if filtered_results['not_found']:\n",
    "        print(\"\\n=== 見つからなかったPMIDの再検索（部分一致） ===\")\n",
    "        print(\"PMIDが部分的に含まれている可能性があるエントリを検索します...\")\n",
    "        \n",
    "        partial_matches = []\n",
    "        for target_pmid in filtered_results['not_found']:\n",
    "            # PMID列が文字列であることを確認し、部分一致を検索\n",
    "            if 'PMID' in included_df.columns:\n",
    "                mask = included_df['PMID'].astype(str).str.contains(target_pmid, na=False, regex=False)\n",
    "                matching_rows = included_df[mask]\n",
    "                \n",
    "                if not matching_rows.empty:\n",
    "                    for _, row in matching_rows.iterrows():\n",
    "                        partial_matches.append({\n",
    "                            'pmid': target_pmid,\n",
    "                            'found_in': row['PMID'],\n",
    "                            'title': row['Title']\n",
    "                        })\n",
    "        \n",
    "        if partial_matches:\n",
    "            print(f\"部分一致するPMIDが{len(partial_matches)}件見つかりました:\")\n",
    "            for i, match in enumerate(partial_matches, 1):\n",
    "                print(f\"{i}. 検索PMID: {match['pmid']}, 見つかったPMID: {match['found_in']}, タイトル: {match['title']}\")\n",
    "        else:\n",
    "            print(\"部分一致するPMIDは見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在の日時を取得（ファイル名用）\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# ステップ1: included_dfから500件をランダムに抽出\n",
    "print(f\"included_dfには{len(included_df)}件のエントリがあります\")\n",
    "\n",
    "# ランダム抽出のシード値を設定（再現性のため）\n",
    "random.seed(42)\n",
    "\n",
    "# 500件をランダムに抽出\n",
    "sample_size = min(500, len(included_df))\n",
    "sampled_indices = random.sample(range(len(included_df)), sample_size)\n",
    "sampled_df = included_df.iloc[sampled_indices].copy()\n",
    "\n",
    "# 残りのエントリを取得\n",
    "remaining_indices = [i for i in range(len(included_df)) if i not in sampled_indices]\n",
    "remaining_df = included_df.iloc[remaining_indices].copy()\n",
    "\n",
    "print(f\"ランダムに{len(sampled_df)}件抽出しました\")\n",
    "print(f\"残りのエントリ数: {len(remaining_df)}\")\n",
    "\n",
    "# ステップ2: 抽出した500件をRayyan用のCSVとして保存\n",
    "# Rayyan形式に合わせて必要なカラムを選択\n",
    "rayyan_columns = ['Title', 'Authors', 'Abstract', 'Journal', 'Year', 'DOI', 'PMID']\n",
    "rayyan_df = sampled_df[rayyan_columns].copy()\n",
    "\n",
    "# Rayyan用のCSVファイル名\n",
    "rayyan_csv_file = f\"rayyan_ml_sample_{sample_size}_{timestamp}.csv\"\n",
    "rayyan_df.to_csv(rayyan_csv_file, index=False, encoding='utf-8')\n",
    "print(f\"Rayyan用に{len(rayyan_df)}件のエントリを {rayyan_csv_file} に保存しました\")\n",
    "\n",
    "# ステップ3: 残りのエントリに対して新しい機械学習フィルターを適用する準備\n",
    "# さらにフィルタリングするためにタイトルとアブストラクトを組み合わせた列を作成\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['Title']),\n",
    "        'abstract': str(row['Abstract'])\n",
    "    })\n",
    "\n",
    "# 機械学習用のtiab列を生成\n",
    "print(\"機械学習用のtiab列を生成中...\")\n",
    "remaining_df['tiab'] = remaining_df.apply(create_json_text, axis=1)\n",
    "\n",
    "# 残りのエントリをCSVとして保存\n",
    "remaining_csv_file = f\"remaining_ml_entries_{timestamp}.csv\"\n",
    "remaining_df.to_csv(remaining_csv_file, index=False, encoding='utf-8')\n",
    "print(f\"残りの{len(remaining_df)}件のエントリを {remaining_csv_file} に保存しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd ml\n",
    "# Load the CSV file into a DataFrame\n",
    "rayyan_decided_df = pd.read_csv('rayyan_decided_3rd500.csv',encoding='cp932')\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(rayyan_decided_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rayyan_decided_dfに対してincludedフラグを作成\n",
    "# 'notes'列に'Included'という文字列が含まれていればTrue\n",
    "rayyan_decided_df['included'] = rayyan_decided_df['notes'].astype(str).str.contains('Included')\n",
    "\n",
    "# 結果の集計を表示\n",
    "print(rayyan_decided_df['included'].value_counts())\n",
    "\n",
    "# 不整合（'included'がTrueなのに'notes'に'Excluded'が含まれる）の件数\n",
    "inconsistency_count = rayyan_decided_df[rayyan_decided_df[\"included\"] == True][\"notes\"].str.contains(\"Excluded\").sum()\n",
    "print(f'不整合の件数: {inconsistency_count}')\n",
    "\n",
    "# タイトルと抄録を組み合わせた新しい列'tiab'を作成\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['title']),\n",
    "        'abstract': str(row['abstract'])\n",
    "    })\n",
    "\n",
    "# 'tiab'列を作成\n",
    "rayyan_decided_df['tiab'] = rayyan_decided_df.apply(create_json_text, axis=1)\n",
    "\n",
    "# 最初の数行を表示して確認\n",
    "print(\"\\n最初の3行のデータ:\")\n",
    "print(rayyan_decided_df[['title', 'abstract', 'notes', 'included', 'tiab']].head(3))\n",
    "\n",
    "# 必要に応じてCSVとして保存\n",
    "# rayyan_decided_df.to_csv('rayyan_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    # データの読み込み\n",
    "    #df = pd.read_csv('your_data.csv')  # tiab列とincluded列を含むデータ\n",
    "\n",
    "    # モデルのトレーニング\n",
    "    model, vectorizer, threshold, best_params = train_final_model(\n",
    "        rayyan_decided_df,\n",
    "        beta_value=4,  # より高い値で再現率を重視\n",
    "        n_trials=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日時をstringsで\n",
    "timestamp = time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "# モデルと関連オブジェクトの保存\n",
    "model_data = {\n",
    "    'model': model,\n",
    "    'vectorizer': vectorizer,\n",
    "    'threshold': threshold,\n",
    "    'best_params': best_params\n",
    "}\n",
    "joblib.dump(model_data, f'{timestamp}_v2_model_data.joblib')\n",
    "print(\"\\nモデルと関連オブジェクトを保存しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = glob.glob('*.joblib')\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import joblib\n",
    "\n",
    "# モデルファイルを検索\n",
    "models = glob.glob('20250311144512_v2_model_data.joblib')\n",
    "print(f\"見つかったモデルファイル: {models}\")\n",
    "\n",
    "if not models:\n",
    "    print(\"joblib ファイルが見つかりません。\")\n",
    "else:\n",
    "    # インデックスを修正 - 最初の要素を使用\n",
    "    model_file = models[0]\n",
    "    print(f\"使用するモデルファイル: {model_file}\")\n",
    "    \n",
    "    try:\n",
    "        # 保存したモデルデータをロード\n",
    "        model_data = joblib.load(model_file)\n",
    "        \n",
    "        # 各オブジェクトの取り出し\n",
    "        loaded_model = model_data['model']\n",
    "        loaded_vectorizer = model_data['vectorizer']\n",
    "        loaded_threshold = model_data['threshold']\n",
    "        loaded_best_params = model_data['best_params']\n",
    "        \n",
    "        print(\"モデルと関連オブジェクトをロードしました。\")\n",
    "        print(f\"モデルの種類: {type(loaded_model).__name__}\")\n",
    "        print(f\"最適な閾値: {loaded_threshold}\")\n",
    "        print(f\"最適なパラメータ: {loaded_best_params}\")\n",
    "    except Exception as e:\n",
    "        print(f\"モデルのロード中にエラーが発生しました: {str(e)}\")\n",
    "        \n",
    "        # ファイルの内容を確認\n",
    "        try:\n",
    "            test_load = joblib.load(model_file)\n",
    "            print(f\"ファイルには次のキーが含まれています: {list(test_load.keys()) if isinstance(test_load, dict) else '辞書ではありません'}\")\n",
    "        except:\n",
    "            print(\"ファイルの内容を確認できませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残っている文献をcsvにしてあったので読み込む\n",
    "remaining_df = pd.read_csv('remaining_ml_entries_20250306155821.csv')\n",
    "\n",
    "# tiab列を作る関数（既に定義済み）\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['Title']),\n",
    "        'abstract': str(row['Abstract'])\n",
    "    })\n",
    "\n",
    "# 機械学習フィルターを適用\n",
    "print(\"機械学習フィルターを適用中...\")\n",
    "filtered_df, class_distribution, probability_stats = predict_and_summarize(\n",
    "    remaining_df, create_json_text, loaded_vectorizer, loaded_model, loaded_threshold\n",
    ")\n",
    "\n",
    "# 「含まれる」と予測されたエントリのみを抽出\n",
    "included_df = filtered_df[filtered_df['prediction'] == 1]\n",
    "print(f\"\\n「含まれる」と予測されたエントリ: {len(included_df)}件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索したいPMIDのリスト（文字列から数字だけを抽出）\n",
    "pmid_list_raw = \"\"\"1888246[pmid] or 2393205[pmid] or 16586387[pmid] or 9395366[pmid] or 8263576[pmid] or 14630621[pmid] or \n",
    "15482728[pmid] or 18486413[pmid] or 19555286[pmid] or 20586653[pmid] or 20299634[pmid] or 20557905[pmid] or \n",
    "21722666[pmid] or 23808716[pmid] or 24856736[pmid] or 18486413[pmid] or 20557905[pmid] or 21083886[pmid] or \n",
    "21684227[pmid] or 22463870[pmid] or 24856736[pmid] or 26656373[pmid] or 26246024[pmid] or 28714287[pmid] or \n",
    "32291611[pmid] or 27023336[pmid] or 33740571[pmid] or 33810788[pmid] or 34992000[pmid] or 34032112[pmid] or \n",
    "34487306[pmid] or 32796184[pmid] or 31581716[pmid] or 35054269[pmid] or 32646430[pmid] or 37620774[pmid] or \n",
    "35351003[pmid] or 33658812[pmid] or 34484472[pmid] or 38396484[pmid] or 30111827[pmid] or 34428529[pmid] or \n",
    "29661634[pmid] or 37519767[pmid] or 34309570[pmid] or 33347714[pmid] or 34162572[pmid] or 37719647[pmid] or \n",
    "34998038[pmid] or 37598221[pmid] or 34983764[pmid] or 37244761[pmid] or 35853298[pmid] or 36292187[pmid] or \n",
    "37562317[pmid] or 31912208[pmid] or 30420241[pmid] or 34209759[pmid] or 29486341[pmid] or 34490306[pmid]\"\"\"\n",
    "\n",
    "# 数字のみを抽出\n",
    "pmid_list = re.findall(r'(\\d+)', pmid_list_raw)\n",
    "\n",
    "# 重複を除去（リスト内に重複PMIDがある）\n",
    "pmid_list = list(set(pmid_list))\n",
    "\n",
    "print(f\"検索するPMID数: {len(pmid_list)}\")\n",
    "\n",
    "# included_dfのPMID列を確認\n",
    "if 'PMID' not in included_df.columns:\n",
    "    print(\"警告: included_dfにPMID列が見つかりません。確認できません。\")\n",
    "else:\n",
    "    # 結果を格納する辞書\n",
    "    filtered_results = {\n",
    "        'found': [],    # フィルター結果に見つかったPMID\n",
    "        'not_found': [] # フィルター結果に見つからなかったPMID\n",
    "    }\n",
    "    \n",
    "    # PMIDの存在確認（直接マッチング）\n",
    "    for target_pmid in pmid_list:\n",
    "        # PMID列で直接一致を確認\n",
    "        mask = included_df['PMID'].astype(str) == target_pmid\n",
    "        if mask.any():\n",
    "            # 見つかった場合、その行の情報を保存\n",
    "            matching_rows = included_df[mask]\n",
    "            for _, row in matching_rows.iterrows():\n",
    "                filtered_results['found'].append({\n",
    "                    'pmid': target_pmid,\n",
    "                    'title': row['Title'],\n",
    "                    'probability': row['probability'] if 'probability' in included_df.columns else 'N/A'\n",
    "                })\n",
    "        else:\n",
    "            # 見つからなかった場合\n",
    "            filtered_results['not_found'].append(target_pmid)\n",
    "    \n",
    "    # 結果を表示\n",
    "    print(\"\\n=== 機械学習フィルター結果内のPMID検索結果 ===\")\n",
    "    print(f\"フィルター後のエントリ数: {len(included_df)}\")\n",
    "    print(f\"検索したPMID数: {len(pmid_list)}\")\n",
    "    print(f\"フィルター結果内に見つかったPMID数: {len(filtered_results['found'])}\")\n",
    "    print(f\"フィルター結果内に見つからなかったPMID数: {len(filtered_results['not_found'])}\")\n",
    "    \n",
    "    # 見つかったPMIDの詳細を表示\n",
    "    if filtered_results['found']:\n",
    "        print(\"\\n=== フィルター結果内に見つかったPMIDの詳細 ===\")\n",
    "        for i, entry in enumerate(filtered_results['found'], 1):\n",
    "            print(f\"\\n--- {i}. PMID: {entry['pmid']} ---\")\n",
    "            print(f\"タイトル: {entry['title']}\")\n",
    "            print(f\"予測確率: {entry['probability']}\")\n",
    "    \n",
    "    # 見つからなかったPMIDのリスト\n",
    "    if filtered_results['not_found']:\n",
    "        print(\"\\n=== フィルター結果内に見つからなかったPMID ===\")\n",
    "        # 10個ずつ表示\n",
    "        for i in range(0, len(filtered_results['not_found']), 10):\n",
    "            chunk = filtered_results['not_found'][i:i+10]\n",
    "            print(\", \".join(chunk))\n",
    "    \n",
    "    # 結果をファイルに保存\n",
    "    with open('filtered_pmid_results.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== フィルター結果内に見つかったPMID ===\\n\")\n",
    "        for entry in filtered_results['found']:\n",
    "            f.write(f\"PMID: {entry['pmid']}, タイトル: {entry['title']}, 予測確率: {entry['probability']}\\n\")\n",
    "        \n",
    "        f.write(\"\\n=== フィルター結果内に見つからなかったPMID ===\\n\")\n",
    "        f.write(\", \".join(filtered_results['not_found']))\n",
    "    \n",
    "    print(\"\\n検索結果をfiltered_pmid_results.txtに保存しました。\")\n",
    "\n",
    "    # PMIDが見つからなかった場合の分析\n",
    "    if filtered_results['not_found']:\n",
    "        print(\"\\n=== 見つからなかったPMIDの再検索（部分一致） ===\")\n",
    "        print(\"PMIDが部分的に含まれている可能性があるエントリを検索します...\")\n",
    "        \n",
    "        partial_matches = []\n",
    "        for target_pmid in filtered_results['not_found']:\n",
    "            # PMID列が文字列であることを確認し、部分一致を検索\n",
    "            if 'PMID' in included_df.columns:\n",
    "                mask = included_df['PMID'].astype(str).str.contains(target_pmid, na=False, regex=False)\n",
    "                matching_rows = included_df[mask]\n",
    "                \n",
    "                if not matching_rows.empty:\n",
    "                    for _, row in matching_rows.iterrows():\n",
    "                        partial_matches.append({\n",
    "                            'pmid': target_pmid,\n",
    "                            'found_in': row['PMID'],\n",
    "                            'title': row['Title']\n",
    "                        })\n",
    "        \n",
    "        if partial_matches:\n",
    "            print(f\"部分一致するPMIDが{len(partial_matches)}件見つかりました:\")\n",
    "            for i, match in enumerate(partial_matches, 1):\n",
    "                print(f\"{i}. 検索PMID: {match['pmid']}, 見つかったPMID: {match['found_in']}, タイトル: {match['title']}\")\n",
    "        else:\n",
    "            print(\"部分一致するPMIDは見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在の日時を取得（ファイル名用）\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# ステップ1: included_dfから500件をランダムに抽出\n",
    "print(f\"included_dfには{len(included_df)}件のエントリがあります\")\n",
    "\n",
    "# ランダム抽出のシード値を設定（再現性のため）\n",
    "random.seed(42)\n",
    "\n",
    "# 500件をランダムに抽出\n",
    "sample_size = min(500, len(included_df))\n",
    "sampled_indices = random.sample(range(len(included_df)), sample_size)\n",
    "sampled_df = included_df.iloc[sampled_indices].copy()\n",
    "\n",
    "# 残りのエントリを取得\n",
    "remaining_indices = [i for i in range(len(included_df)) if i not in sampled_indices]\n",
    "remaining_df = included_df.iloc[remaining_indices].copy()\n",
    "\n",
    "print(f\"ランダムに{len(sampled_df)}件抽出しました\")\n",
    "print(f\"残りのエントリ数: {len(remaining_df)}\")\n",
    "\n",
    "# ステップ2: 抽出した500件をRayyan用のCSVとして保存\n",
    "# Rayyan形式に合わせて必要なカラムを選択\n",
    "rayyan_columns = ['Title', 'Authors', 'Abstract', 'Journal', 'Year', 'DOI', 'PMID']\n",
    "rayyan_df = sampled_df[rayyan_columns].copy()\n",
    "\n",
    "# Rayyan用のCSVファイル名\n",
    "rayyan_csv_file = f\"rayyan_ml_sample_{sample_size}_{timestamp}.csv\"\n",
    "rayyan_df.to_csv(rayyan_csv_file, index=False, encoding='utf-8')\n",
    "print(f\"Rayyan用に{len(rayyan_df)}件のエントリを {rayyan_csv_file} に保存しました\")\n",
    "\n",
    "# ステップ3: 残りのエントリに対して新しい機械学習フィルターを適用する準備\n",
    "# さらにフィルタリングするためにタイトルとアブストラクトを組み合わせた列を作成\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['Title']),\n",
    "        'abstract': str(row['Abstract'])\n",
    "    })\n",
    "\n",
    "# 機械学習用のtiab列を生成\n",
    "print(\"機械学習用のtiab列を生成中...\")\n",
    "remaining_df['tiab'] = remaining_df.apply(create_json_text, axis=1)\n",
    "\n",
    "# 残りのエントリをCSVとして保存\n",
    "remaining_csv_file = f\"remaining_ml_entries_{timestamp}.csv\"\n",
    "remaining_df.to_csv(remaining_csv_file, index=False, encoding='utf-8')\n",
    "print(f\"残りの{len(remaining_df)}件のエントリを {remaining_csv_file} に保存しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在の日時を取得（ファイル名用）\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# included_dfのサイズを確認\n",
    "print(f\"included_dfには{len(included_df)}件のエントリがあります\")\n",
    "\n",
    "# ランダムシャッフルのシード値を設定（再現性のため）\n",
    "random.seed(42)\n",
    "\n",
    "# データフレームのインデックスをシャッフル\n",
    "all_indices = list(range(len(included_df)))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "# 指定されたサイズで分割\n",
    "group_sizes = [430, 430, 430, 430, 430, 420]\n",
    "\n",
    "# 実際のデータサイズと確認\n",
    "total_requested = sum(group_sizes)\n",
    "if total_requested > len(included_df):\n",
    "    print(f\"警告: 要求された合計サイズ({total_requested})がデータサイズ({len(included_df)})を超えています\")\n",
    "    print(\"最後のグループのサイズを調整します\")\n",
    "    # 最後のグループのサイズを調整\n",
    "    group_sizes[-1] = max(0, len(included_df) - sum(group_sizes[:-1]))\n",
    "\n",
    "print(f\"指定されたグループサイズ: {group_sizes}\")\n",
    "print(f\"合計: {sum(group_sizes)}件\")\n",
    "\n",
    "# 各グループのインデックスを割り当て\n",
    "group_indices = []\n",
    "start_idx = 0\n",
    "\n",
    "for i, size in enumerate(group_sizes):\n",
    "    end_idx = start_idx + size\n",
    "    if end_idx > len(all_indices):\n",
    "        end_idx = len(all_indices)\n",
    "    group_indices.append(all_indices[start_idx:end_idx])\n",
    "    start_idx = end_idx\n",
    "    \n",
    "    # インデックスが足りなくなったら終了\n",
    "    if start_idx >= len(all_indices):\n",
    "        break\n",
    "\n",
    "# 実際に作成されたグループ数\n",
    "actual_groups = len(group_indices)\n",
    "print(f\"実際に作成されたグループ数: {actual_groups}\")\n",
    "\n",
    "# 各グループのサイズを確認\n",
    "for i, indices in enumerate(group_indices, 1):\n",
    "    print(f\"グループ{i}のサイズ: {len(indices)}件\")\n",
    "\n",
    "# Rayyan形式用のカラム\n",
    "rayyan_columns = ['Title', 'Authors', 'Abstract', 'Journal', 'Year', 'DOI', 'PMID']\n",
    "\n",
    "# 各グループを別々のCSVファイルに保存\n",
    "for i, indices in enumerate(group_indices, 1):\n",
    "    # グループのデータフレームを作成\n",
    "    group_df = included_df.iloc[indices].copy()\n",
    "    \n",
    "    # Rayyan用のカラムを選択\n",
    "    if all(col in group_df.columns for col in rayyan_columns):\n",
    "        rayyan_df = group_df[rayyan_columns].copy()\n",
    "    else:\n",
    "        # カラムが見つからない場合は、利用可能なすべてのカラムを使用\n",
    "        rayyan_df = group_df.copy()\n",
    "        print(f\"警告: グループ{i}では一部のRayyan列が見つかりません。利用可能なすべての列を使用します。\")\n",
    "    \n",
    "    # CSVファイル名\n",
    "    csv_file = f\"rayyan_group_{i}_of_{actual_groups}_{timestamp}.csv\"\n",
    "    \n",
    "    # CSVとして保存\n",
    "    rayyan_df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "    print(f\"グループ{i}（{len(rayyan_df)}件）を {csv_file} に保存しました\")\n",
    "\n",
    "print(\"\\n処理完了！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rispy\n",
    "\n",
    "# ファイルパス\n",
    "reference_path = \"path/to/your/references.ris\"\n",
    "citation_path = \"path/to/your/references.ris\"\n",
    "\n",
    "# RISファイルを読み込む関数\n",
    "def read_ris_to_entries(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as ris_file:\n",
    "        entries = rispy.load(ris_file)\n",
    "    return entries\n",
    "\n",
    "# 両方のRISファイルを読み込む\n",
    "reference_entries = read_ris_to_entries(reference_path)\n",
    "citation_entries = read_ris_to_entries(citation_path)\n",
    "\n",
    "# 両方のエントリを結合\n",
    "all_entries = reference_entries + citation_entries\n",
    "\n",
    "# エントリ数を表示\n",
    "print(f\"参照文献数: {len(reference_entries)}\")\n",
    "print(f\"引用文献数: {len(citation_entries)}\")\n",
    "print(f\"合計エントリ数: {len(all_entries)}\")\n",
    "\n",
    "# DataFrameに変換\n",
    "df = pd.DataFrame(all_entries)\n",
    "\n",
    "# DataFrameの基本情報を表示\n",
    "print(\"\\nDataFrameの列:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\n最初の数行:\")\n",
    "print(df.head())\n",
    "\n",
    "# BibTeXに変換したい場合は以下のようにします\n",
    "# bibtex_db = bibtexparser.bibdatabase.BibDatabase()\n",
    "# bibtex_db.entries = all_entries\n",
    "# bibtex_str = bibtexparser.dumps(bibtex_db)\n",
    "# with open('combined_references.bib', 'w', encoding='utf-8') as bibtex_file:\n",
    "#     bibtex_file.write(bibtex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 機械学習フィルターを適用\n",
    "import json\n",
    "\n",
    "def create_json_text(row):\n",
    "    return json.dumps({\n",
    "        'title': str(row['title']),\n",
    "        'abstract': str(row['abstract'])\n",
    "    })\n",
    "\n",
    "print(\"機械学習フィルターを適用中...\")\n",
    "filtered_df, class_distribution, probability_stats = predict_and_summarize(\n",
    "    df, create_json_text, loaded_vectorizer, loaded_model, loaded_threshold\n",
    ")\n",
    "\n",
    "# 「含まれる」と予測されたエントリのみを抽出\n",
    "included_df = filtered_df[filtered_df['prediction'] == 1]\n",
    "print(f\"\\n「含まれる」と予測されたエントリ: {len(included_df)}件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rispy\n",
    "from datetime import datetime\n",
    "\n",
    "# フィルタリング後のDataFrameをRISフォーマットで保存する\n",
    "def save_to_ris(dataframe, output_path):\n",
    "    # DataFrameの各行をrisエントリに変換\n",
    "    ris_entries = []\n",
    "    \n",
    "    for _, row in dataframe.iterrows():\n",
    "        # 行をdictに変換（NaNを除外）\n",
    "        entry = row.dropna().to_dict()\n",
    "        ris_entries.append(entry)\n",
    "    \n",
    "    # RISファイルとして保存\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        rispy.dump(ris_entries, f)\n",
    "    \n",
    "    print(f\"保存完了: {output_path} ({len(ris_entries)}件のエントリ)\")\n",
    "\n",
    "# フィルタリング後のDataFrameをRISファイルとして保存\n",
    "output_path = \"path/to/your/references.ris\"\n",
    "save_to_ris(included_df, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
