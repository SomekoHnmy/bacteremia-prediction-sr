{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9dqhKYsAPW4"
      },
      "source": [
        "各JSONファイルに対して、フルテキストのみを抽出し、ファイル名、フルテキストのdataframeを作成\n",
        "フルテキストをGPT、Geminiに投げて、構造化した組み入れ・除外基準を返してもらう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MpltXtjDh0W"
      },
      "source": [
        "まずはpdf extraction.ipynbで、1つのフォルダにまとめたpdfから情報を抜き出してjsonファイルを作る。ADobeのAPIを使用する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7lkPGrpaDtqJ",
        "outputId": "06363e1e-4e5c-45cf-dca0-abb5d6c50070"
      },
      "outputs": [],
      "source": [
        "pip install pdfservices-sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCaoq2rkE07k",
        "outputId": "96292f1a-39ff-40ea-a781-1406aaffc8b9"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# ZIPファイルのパス\n",
        "zip_path = \"your_zip_file_path.zip\"  # ここに実際のZIPファイルのパスを指定してください\n",
        "\n",
        "# パスが本当にZIPファイルなのか確認\n",
        "if os.path.isdir(zip_path):\n",
        "    print(\"これはディレクトリです。ZIPファイルではありません。\")\n",
        "    # ディレクトリ内のファイルを確認\n",
        "    files = os.listdir(zip_path)\n",
        "    print(f\"ディレクトリ内のファイル: {files[:10]}\")  # 最初の10個だけ表示\n",
        "elif os.path.isfile(zip_path):\n",
        "    print(\"これはファイルです。\")\n",
        "    # おそらく拡張子が抜けている可能性があります\n",
        "    if not zip_path.endswith('.zip'):\n",
        "        print(\"ZIPファイルの拡張子がありません。\")\n",
        "        zip_path = zip_path + '.zip'\n",
        "        print(f\"拡張子を追加したパス: {zip_path}\")\n",
        "    \n",
        "    # ZIPファイルの中身を確認\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            # ZIPファイル内のファイル一覧を取得\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(f\"ZIP内のファイル数: {len(file_list)}\")\n",
        "            print(f\"最初の10個のファイル: {file_list[:10]}\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"無効なZIPファイルです。\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"ファイルが見つかりません。\")\n",
        "else:\n",
        "    print(\"指定されたパスが存在しません。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# パスの設定\n",
        "zip_path = \"your_zip_file_path.zip\"  # ここに実際のZIPファイルのパスを指定してください\n",
        "target_dir = \"extracted_pdfs\"  # 抽出先のディレクトリ名\n",
        "\n",
        "# 対象ディレクトリが存在しない場合は作成\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "    print(f\"ディレクトリを作成しました: {target_dir}\")\n",
        "else:\n",
        "    print(f\"ディレクトリはすでに存在します: {target_dir}\")\n",
        "\n",
        "# ZIPファイルを開いてPDFファイルを抽出\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # ZIPファイル内のファイル一覧を取得\n",
        "        file_list = [f for f in zip_ref.namelist() if f.lower().endswith('.pdf')]\n",
        "        \n",
        "        print(f\"PDFファイル数: {len(file_list)}\")\n",
        "        \n",
        "        # 進捗表示のためのカウンタ\n",
        "        counter = 0\n",
        "        \n",
        "        # PDFファイルを抽出\n",
        "        for file_name in file_list:\n",
        "            try:\n",
        "                # ファイル名のみを取得 (パスを除去)\n",
        "                base_name = os.path.basename(file_name)\n",
        "                \n",
        "                # ファイル名を「-」の前の部分だけに短縮する\n",
        "                name_parts = os.path.splitext(base_name)  # 拡張子を分離\n",
        "                file_name_without_ext = name_parts[0]\n",
        "                extension = name_parts[1]  # 拡張子 (.pdf)\n",
        "                \n",
        "                # ハイフンで分割して最初の部分だけを取得\n",
        "                if '-' in file_name_without_ext:\n",
        "                    shortened_name = file_name_without_ext.split('-')[0].strip()\n",
        "                    base_name = shortened_name + extension\n",
        "                else:\n",
        "                    # ハイフンがない場合はそのまま使用\n",
        "                    base_name = file_name_without_ext + extension\n",
        "                \n",
        "                # 対象パス\n",
        "                target_path = os.path.join(target_dir, base_name)\n",
        "                \n",
        "                # ファイルを抽出して保存\n",
        "                with zip_ref.open(file_name) as source, open(target_path, 'wb') as target:\n",
        "                    shutil.copyfileobj(source, target)\n",
        "                \n",
        "                counter += 1\n",
        "                if counter % 50 == 0:  # 50ファイルごとに進捗を表示\n",
        "                    print(f\"進捗: {counter}/{len(file_list)} ファイルを処理しました\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"ファイル {file_name} の処理中にエラーが発生しました: {e}\")\n",
        "        \n",
        "        print(f\"完了しました。{counter}/{len(file_list)} ファイルを抽出しました。\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"エラーが発生しました: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6u7XIWRJB6M",
        "outputId": "b9578598-6ca3-4dad-89bb-eb5d36764fde"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# フォルダのパス\n",
        "folder_path = \"extracted_pdfs\"  # 抽出したPDFファイルが保存されているフォルダのパス\n",
        "\n",
        "# フォルダが存在するか確認\n",
        "if not os.path.exists(folder_path):\n",
        "    print(f\"エラー: フォルダが存在しません: {folder_path}\")\n",
        "else:\n",
        "    # フォルダ内のファイルをリストアップ\n",
        "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "    \n",
        "    # ファイル数を表示\n",
        "    print(f\"フォルダ内のファイル総数: {len(files)}\")\n",
        "    \n",
        "    # PDFファイルの数を確認\n",
        "    pdf_files = [f for f in files if f.lower().endswith('.pdf')]\n",
        "    print(f\"PDFファイルの数: {len(pdf_files)}\")\n",
        "    \n",
        "    # 最初の10個のファイル名を表示\n",
        "    if files:\n",
        "        print(\"\\n最初の10個のファイル名:\")\n",
        "        for i, file in enumerate(files[:10]):\n",
        "            print(f\"{i+1}. {file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3ifEh-aDg--",
        "outputId": "cf52cd92-49c6-470b-9ada-8dcacc612d86"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "# 絶対パスを使用\n",
        "folder_path = \"extracted_pdfs\"  # 抽出したPDFファイルが保存されているフォルダのパス\n",
        "pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
        "\n",
        "print(f\"PDFファイルの数: {len(pdf_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 現在の作業ディレクトリを取得して表示\n",
        "current_directory = os.getcwd()\n",
        "print(f\"現在の作業ディレクトリ: {current_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# 作業ディレクトリを設定\n",
        "target_directory = \"extracted_pdfs\"  # 抽出したPDFファイルが保存されているフォルダのパス\n",
        "os.chdir(target_directory)\n",
        "\n",
        "# 設定後の作業ディレクトリを確認\n",
        "current_directory = os.getcwd()\n",
        "print(f\"現在の作業ディレクトリ: {current_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN-3cNNLGx3S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
        "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
        "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
        "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
        "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
        "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
        "from adobe.pdfservices.operation.pdfjobs.jobs.extract_pdf_job import ExtractPDFJob\n",
        "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_element_type import ExtractElementType\n",
        "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_renditions_element_type import ExtractRenditionsElementType\n",
        "from adobe.pdfservices.operation.pdfjobs.params.extract_pdf.extract_pdf_params import ExtractPDFParams\n",
        "from adobe.pdfservices.operation.pdfjobs.result.extract_pdf_result import ExtractPDFResult\n",
        "\n",
        "class ExtractTextTableInfoWithFiguresTablesRenditionsFromPDF:\n",
        "    def __init__(self, pdf_file):\n",
        "        try:\n",
        "            file = open(pdf_file, 'rb')\n",
        "            input_stream = file.read()\n",
        "            file.close()\n",
        "\n",
        "            # Initial setup, create credentials instance\n",
        "            credentials = ServicePrincipalCredentials(\n",
        "                client_id=\"your_client_id\",\n",
        "                client_secret=\"your_client_secret\"  # Replace with your actual client secret\n",
        "            )\n",
        "\n",
        "            # Creates a PDF Services instance\n",
        "            pdf_services = PDFServices(credentials=credentials)\n",
        "\n",
        "            # Creates an asset(s) from source file(s) and upload\n",
        "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
        "\n",
        "            # Create parameters for the job\n",
        "            extract_pdf_params = ExtractPDFParams(\n",
        "                elements_to_extract=[ExtractElementType.TEXT, ExtractElementType.TABLES],\n",
        "                elements_to_extract_renditions=[ExtractRenditionsElementType.TABLES, ExtractRenditionsElementType.FIGURES],\n",
        "            )\n",
        "\n",
        "            # Creates a new job instance\n",
        "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
        "\n",
        "            # Submit the job and gets the job result\n",
        "            location = pdf_services.submit(extract_pdf_job)\n",
        "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
        "\n",
        "            # Get content from the resulting asset(s)\n",
        "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
        "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
        "\n",
        "            # Creates an output stream and copy stream asset's content to it\n",
        "            output_file_path = \"your_output_file_path.zip\"  # Specify your output file path here\n",
        "            with open(output_file_path, \"wb\") as file:\n",
        "                file.write(stream_asset.get_input_stream())\n",
        "\n",
        "            # ---- ここからフォルダへの解凍処理を追加 ----\n",
        "            # PDF ファイルと同じフォルダ名(拡張子除く)を作成して、そこに解凍する\n",
        "            folder_name = os.path.splitext(os.path.basename(pdf_file))[0]  # PDF と同名のフォルダ名\n",
        "            output_folder_path = os.path.join(os.path.dirname(pdf_file), folder_name)\n",
        "\n",
        "            # 同名のフォルダが存在していない場合のみ解凍を行う\n",
        "            if os.path.exists(output_folder_path):\n",
        "                logging.info(f\"フォルダ '{output_folder_path}' は既に存在しています。解凍をスキップします。\")\n",
        "            else:\n",
        "                os.makedirs(output_folder_path, exist_ok=True)\n",
        "                with zipfile.ZipFile(output_file_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(output_folder_path)\n",
        "                logging.info(f\"zipファイルを解凍して '{output_folder_path}' に保存しました。\")\n",
        "            # ---- ここまで追加 ----\n",
        "\n",
        "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
        "            logging.exception(f'Exception encountered while executing operation: {e}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amNmI5FaLdfc",
        "outputId": "a6720147-fb94-4652-c087-aaa6ab918a54"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
        "        ExtractTextTableInfoWithFiguresTablesRenditionsFromPDF(pdf_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PDFファイル一覧を取得\n",
        "pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
        "print(f\"合計 {len(pdf_files)} 個のPDFファイルを検出しました。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import logging\n",
        "\n",
        "# ログ設定\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# フォルダのパス\n",
        "folder_path = \"extracted_pdfs\"  # 抽出したPDFファイルが保存されているフォルダのパス\n",
        "\n",
        "# PDFファイル一覧を取得\n",
        "pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
        "logging.info(f\"合計 {len(pdf_files)} 個のPDFファイルを検出しました。\")\n",
        "\n",
        "# 既に処理済みのファイルを確認（同名のフォルダが存在するか）\n",
        "processed_files = []\n",
        "remaining_files = []\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    folder_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
        "    output_folder_path = os.path.join(os.path.dirname(pdf_file), folder_name)\n",
        "    \n",
        "    if os.path.exists(output_folder_path):\n",
        "        processed_files.append(pdf_file)\n",
        "    else:\n",
        "        remaining_files.append(pdf_file)\n",
        "\n",
        "print(f\"処理済み: {len(processed_files)} ファイル\")\n",
        "print(f\"未処理: {len(remaining_files)} ファイル\")\n",
        "\n",
        "# 詳細情報を表示（オプション）\n",
        "print(\"\\n処理済みファイルの例（最初の5つ）:\")\n",
        "for i, file in enumerate(processed_files[:5]):\n",
        "    print(f\"{i+1}. {os.path.basename(file)}\")\n",
        "\n",
        "print(\"\\n未処理ファイルの例（最初の5つ）:\")\n",
        "for i, file in enumerate(remaining_files[:5]):\n",
        "    print(f\"{i+1}. {os.path.basename(file)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#API key 変えて再挑戦\n",
        "\n",
        "class ExtractTextTableInfoWithFiguresTablesRenditionsFromPDF:\n",
        "    def __init__(self, pdf_file):\n",
        "        try:\n",
        "            file = open(pdf_file, 'rb')\n",
        "            input_stream = file.read()\n",
        "            file.close()\n",
        "\n",
        "            # Initial setup, create credentials instance\n",
        "            credentials = ServicePrincipalCredentials(\n",
        "                client_id='your_client_id',  # Replace with your actual client ID\n",
        "                client_secret=  'your_client_secret'  # Replace with your actual client secret\n",
        "            )\n",
        "\n",
        "            # Creates a PDF Services instance\n",
        "            pdf_services = PDFServices(credentials=credentials)\n",
        "\n",
        "            # Creates an asset(s) from source file(s) and upload\n",
        "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.PDF)\n",
        "\n",
        "            # Create parameters for the job\n",
        "            extract_pdf_params = ExtractPDFParams(\n",
        "                elements_to_extract=[ExtractElementType.TEXT, ExtractElementType.TABLES],\n",
        "                elements_to_extract_renditions=[ExtractRenditionsElementType.TABLES, ExtractRenditionsElementType.FIGURES],\n",
        "            )\n",
        "\n",
        "            # Creates a new job instance\n",
        "            extract_pdf_job = ExtractPDFJob(input_asset=input_asset, extract_pdf_params=extract_pdf_params)\n",
        "\n",
        "            # Submit the job and gets the job result\n",
        "            location = pdf_services.submit(extract_pdf_job)\n",
        "            pdf_services_response = pdf_services.get_job_result(location, ExtractPDFResult)\n",
        "\n",
        "            # Get content from the resulting asset(s)\n",
        "            result_asset: CloudAsset = pdf_services_response.get_result().get_resource()\n",
        "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
        "\n",
        "            # Creates an output stream and copy stream asset's content to it\n",
        "            output_file_path = 'extractTextTableInfoWithFiguresTablesRenditionsFromPDF.zip'\n",
        "            with open(output_file_path, \"wb\") as file:\n",
        "                file.write(stream_asset.get_input_stream())\n",
        "\n",
        "            # ---- ここからフォルダへの解凍処理を追加 ----\n",
        "            # PDF ファイルと同じフォルダ名(拡張子除く)を作成して、そこに解凍する\n",
        "            folder_name = os.path.splitext(os.path.basename(pdf_file))[0]  # PDF と同名のフォルダ名\n",
        "            output_folder_path = os.path.join(os.path.dirname(pdf_file), folder_name)\n",
        "\n",
        "            # 同名のフォルダが存在していない場合のみ解凍を行う\n",
        "            if os.path.exists(output_folder_path):\n",
        "                logging.info(f\"フォルダ '{output_folder_path}' は既に存在しています。解凍をスキップします。\")\n",
        "            else:\n",
        "                os.makedirs(output_folder_path, exist_ok=True)\n",
        "                with zipfile.ZipFile(output_file_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(output_folder_path)\n",
        "                logging.info(f\"zipファイルを解凍して '{output_folder_path}' に保存しました。\")\n",
        "            # ---- ここまで追加 ----\n",
        "\n",
        "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
        "            logging.exception(f'Exception encountered while executing operation: {e}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#remaining_filesに適用\n",
        "from tqdm import tqdm\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for pdf_file in tqdm(remaining_files, desc=\"Processing PDFs\"):\n",
        "        ExtractTextTableInfoWithFiguresTablesRenditionsFromPDF(pdf_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzVY4EU3APXA",
        "outputId": "c16c4f4e-218b-47e9-afe9-ba47a35d0108"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "jsons = glob.glob(\"fulltext/*/*.json\")\n",
        "print(len(jsons))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9HmaVt5APXB"
      },
      "outputs": [],
      "source": [
        "# jsonファイルを読み込む\n",
        "import json\n",
        "with open(jsons[0]) as f:\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMbVvEUpAPXB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove extra whitespace and normalize spacing in the given text.\n",
        "    \"\"\"\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "def should_start_new_segment(element: Dict[str, Any]) -> bool:\n",
        "    \"\"\"\n",
        "    Determine whether a new segment should start based on element attributes.\n",
        "    \"\"\"\n",
        "    path = element.get('Path', '')\n",
        "    text = element.get('Text', '')\n",
        "\n",
        "    return (\n",
        "        path.endswith(('H1', 'Title')) or\n",
        "        'ABSTRACT' in text\n",
        "    )\n",
        "\n",
        "\n",
        "def extract_text_segments(document_data: Dict[str, Any]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract and segment text content from PDF data while removing formatting information.\n",
        "    \"\"\"\n",
        "    segments: List[str] = []\n",
        "    current_segment: List[str] = []\n",
        "\n",
        "    elements = document_data.get('elements', [])\n",
        "\n",
        "    for element in elements:\n",
        "        # Skip non-text elements or empty text\n",
        "        if 'Text' not in element or not element['Text'].strip():\n",
        "            continue\n",
        "\n",
        "        # Skip figure captions and other non-content elements\n",
        "        if element.get('Path', '').endswith(('Figure', 'Lbl')):\n",
        "            continue\n",
        "\n",
        "        text = clean_text(element['Text'])\n",
        "\n",
        "        # Start new segment if needed\n",
        "        if should_start_new_segment(element):\n",
        "            if current_segment:\n",
        "                segments.append(' '.join(current_segment))\n",
        "                current_segment = []\n",
        "            current_segment.append(text)\n",
        "        else:\n",
        "            current_segment.append(text)\n",
        "\n",
        "    if current_segment:\n",
        "        segments.append(' '.join(current_segment))\n",
        "\n",
        "    return segments\n",
        "\n",
        "\n",
        "def process_pdf_content(document_content: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Process PDF content (JSON formatted string) and return segmented text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pdf_data = json.loads(document_content)\n",
        "        segments = extract_text_segments(pdf_data)\n",
        "        return segments\n",
        "    except json.JSONDecodeError:\n",
        "        return [\"Error: Invalid JSON data\"]\n",
        "    except Exception as e:\n",
        "        return [f\"Error processing document: {str(e)}\"]\n",
        "\n",
        "\n",
        "def read_json_file(file_path: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Safely read JSON file content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file '{file_path}': {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_folder_name(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract folder name from file path, specifically the one after 'fulltext'.\n",
        "    \"\"\"\n",
        "    normalized_path = os.path.normpath(file_path)\n",
        "    path_parts = normalized_path.split(os.sep)\n",
        "    try:\n",
        "        fulltext_index = path_parts.index('fulltext')\n",
        "        if len(path_parts) > fulltext_index + 1:\n",
        "            return path_parts[fulltext_index + 1]\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "\n",
        "def create_segments_dataframe(json_paths: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a DataFrame such that each folder corresponds to one row,\n",
        "    and each row stores a list of segments in the 'segments' column.\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    for json_path in json_paths:\n",
        "        folder_name = extract_folder_name(json_path)\n",
        "        content = read_json_file(json_path)\n",
        "\n",
        "        # フォルダ単位で segments をまとめる\n",
        "        segments = []\n",
        "        if content:\n",
        "            segments = process_pdf_content(content)\n",
        "\n",
        "        all_data.append({\n",
        "            'folder_name': folder_name,\n",
        "            # まとめた segment をひとつのリストとして保存\n",
        "            'segments': segments\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "\n",
        "def save_segments_to_csv(\n",
        "    json_paths: List[str],\n",
        "    output_path: str = 'full_text_without_tab_and_fig_output.csv'\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process JSON files and save segments to CSV.\n",
        "    \"\"\"\n",
        "    df = create_segments_dataframe(json_paths)\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "    print(f\"Saved segments to {output_path}\")\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ObQCsqpAPXC",
        "outputId": "00f6a8ca-eb17-4d0f-f1f5-e56c10018e09"
      },
      "outputs": [],
      "source": [
        "# Usage example (will only run when this file is executed directly).\n",
        "if __name__ == \"__main__\":\n",
        "    df = save_segments_to_csv(jsons)\n",
        "    print(f\"Created DataFrame with {len(df)} rows\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_dB0sxFNgPp"
      },
      "source": [
        "これでフルテキストを抽出したcsvができた。以下はjudgementのipynbに続く"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSwG6CH7APXC",
        "outputId": "0ffb7e76-172c-4a0c-b0b2-3ce623d374ff"
      },
      "outputs": [],
      "source": [
        "type(df[\"segments\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ4pk02X1ieA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcjBWxhtAPXD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"full_text_without_tab_and_fig_output.csv\", encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBTdGmbt13GR",
        "outputId": "203aa6dc-6135-4468-af6f-eed22943695e"
      },
      "outputs": [],
      "source": [
        "df.head()\n",
        "\n",
        "df[\"segments\"] = df[\"segments\"].apply(str)\n",
        "\n",
        "type(df[\"segments\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzJLC0E117z3"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean text by removing unnecessary newlines and normalizing spaces.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Input text with unwanted newlines\n",
        "\n",
        "    Returns:\n",
        "    str: Cleaned text with proper spacing\n",
        "    \"\"\"\n",
        "    # Remove single newline characters\n",
        "    text = text.replace('\\n', '')\n",
        "\n",
        "    # Normalize spaces (remove multiple spaces)\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAiwtugw1_r6"
      },
      "outputs": [],
      "source": [
        "df['segments'] = df['segments'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fb95IPfI2Aov",
        "outputId": "6a45239e-fdf5-40ff-d31b-d33f93707804"
      },
      "outputs": [],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGE2WL-R5c7H",
        "outputId": "61ce8ad0-fccd-4d1c-8174-1ae489e417c8"
      },
      "outputs": [],
      "source": [
        "pip install anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import Optional, List\n",
        "from dataclasses import dataclass\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI  # Anthropic の代わりに OpenAI SDK を使用\n",
        "\n",
        "@dataclass\n",
        "class FullTextScreeningResult:\n",
        "    judgement: Optional[bool]  # 含める(True)/含めない(False)\n",
        "    reason: Optional[str]       # 理由\n",
        "    error: Optional[str] = None # APIエラーなどがあった場合\n",
        "\n",
        "\n",
        "class FullTextDecision(BaseModel):\n",
        "    judgement: bool\n",
        "    reason: str\n",
        "\n",
        "\n",
        "def get_fulltext_completion(\n",
        "    system_prompt: str,\n",
        "    prompt: str,\n",
        "    client: OpenAI,  # OpenAI クライアントを使用\n",
        "    model: str = \"anthropic/claude-3-7-sonnet\",  # OpenRouter のモデル名\n",
        "    temperature: float = 0.0\n",
        ") -> FullTextDecision:\n",
        "    \"\"\"\n",
        "    OpenRouter経由でClaudeにフルテキストを読ませ、含める／含めないの判断と理由を返してもらう関数。\n",
        "    \"\"\"\n",
        "    # システムプロンプトでフォーマットの指示を行う\n",
        "    enhanced_system_prompt = f\"\"\"\n",
        "    {system_prompt}\n",
        "    \n",
        "    以下のテキストを評価し、指定された基準に基づいて含めるか除外するかを判断してください。\n",
        "    結果は以下のXMLフォーマットで必ず返してください:\n",
        "\n",
        "    <judgement>true または false</judgement>\n",
        "    <reason>判断の詳細な理由</reason>\n",
        "    \"\"\"\n",
        "\n",
        "    # OpenRouter APIを呼び出し (OpenAI の形式を使用)\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": enhanced_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        extra_headers={\n",
        "            \"HTTP-Referer\": \"https://yourwebsite.com\",  # 必要に応じて変更\n",
        "            \"X-Title\": \"SystematicReviewApp\",          # 必要に応じて変更\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # レスポンスからテキストを抽出\n",
        "    response_content = response.choices[0].message.content\n",
        "\n",
        "    # XMLタグから判断と理由を抽出\n",
        "    try:\n",
        "        # 判断を抽出\n",
        "        judgement_start = response_content.find(\"<judgement>\") + len(\"<judgement>\")\n",
        "        judgement_end = response_content.find(\"</judgement>\")\n",
        "        judgement_text = response_content[judgement_start:judgement_end].strip().lower()\n",
        "        judgement = judgement_text == \"true\"\n",
        "\n",
        "        # 理由を抽出\n",
        "        reason_start = response_content.find(\"<reason>\") + len(\"<reason>\")\n",
        "        reason_end = response_content.find(\"</reason>\")\n",
        "        reason = response_content[reason_start:reason_end].strip()\n",
        "\n",
        "        return FullTextDecision(judgement=judgement, reason=reason)\n",
        "    except Exception as e:\n",
        "        # XMLタグがない場合は、テキスト全体を理由として扱い、判断をFalseとする\n",
        "        print(f\"Error parsing XML response: {e}\")\n",
        "        print(f\"Raw response: {response_content}\")\n",
        "\n",
        "        # シンプルなヒューリスティックで判断を試みる\n",
        "        judgement = \"含める\" in response_content.lower() or \"include\" in response_content.lower()\n",
        "\n",
        "        return FullTextDecision(\n",
        "            judgement=judgement,\n",
        "            reason=\"APIレスポンスからの抽出に失敗しました。テキスト全体: \" + response_content[:200] + \"...\"\n",
        "        )\n",
        "\n",
        "\n",
        "def process_fulltext_abstracts(\n",
        "    df: pd.DataFrame,\n",
        "    system_prompt: str,\n",
        "    client: OpenAI,  # OpenAI クライアントを使用\n",
        "    text_column: str = \"segments\",  # フルテキストが入っている列名\n",
        "    max_retries: int = 3,\n",
        "    retry_delay: int = 5,\n",
        "    model: str = \"anthropic/claude-3-7-sonnet\"  # OpenRouter のモデル名\n",
        ") -> List[FullTextScreeningResult]:\n",
        "    \"\"\"\n",
        "    DataFrameの各行に含まれるフルテキストをClaudeに投げて、\n",
        "    含める／含めない＆理由をまとめて返す。\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        text_to_screen = row.get(text_column, \"\")\n",
        "        current_result = None\n",
        "        error_message = None\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # Claudeに問い合わせ\n",
        "                decision = get_fulltext_completion(\n",
        "                    system_prompt=system_prompt,\n",
        "                    prompt=text_to_screen,\n",
        "                    client=client,\n",
        "                    model=model\n",
        "                )\n",
        "                current_result = FullTextScreeningResult(\n",
        "                    judgement=decision.judgement,\n",
        "                    reason=decision.reason\n",
        "                )\n",
        "                break  # 成功したらリトライループを抜ける\n",
        "\n",
        "            except Exception as e:\n",
        "                error_message = str(e)\n",
        "                print(f\"[{i}] Error (attempt {attempt+1}/{max_retries}): {error_message}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    print(\"Max retries reached. Moving to next.\")\n",
        "\n",
        "        # current_resultに成功結果が入っていなければ、エラーとして記録\n",
        "        if not current_result:\n",
        "            current_result = FullTextScreeningResult(\n",
        "                judgement=None,\n",
        "                reason=None,\n",
        "                error=error_message\n",
        "            )\n",
        "\n",
        "        results.append(current_result)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def process_fulltext_screening(\n",
        "    df: pd.DataFrame,\n",
        "    system_prompt: str,\n",
        "    client: OpenAI,  # OpenAI クライアントを使用\n",
        "    text_column: str = \"segments\",\n",
        "    model: str = \"anthropic/claude-3-7-sonnet\"  # OpenRouter のモデル名\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    フルテキストスクリーニングを実行し、結果を元のDataFrameに追加して返す\n",
        "\n",
        "    Args:\n",
        "        df: 入力DataFrame\n",
        "        system_prompt: システムプロンプト\n",
        "        client: OpenAI APIクライアント\n",
        "        text_column: テキストが含まれる列名\n",
        "        model: 使用するモデル名\n",
        "\n",
        "    Returns:\n",
        "        結果を追加したDataFrame\n",
        "    \"\"\"\n",
        "    # 元のdfを変更しないようにコピー\n",
        "    df = df.copy()\n",
        "\n",
        "    # 各テキストに対してスクリーニングを実行\n",
        "    results = process_fulltext_abstracts(\n",
        "        df=df,\n",
        "        system_prompt=system_prompt,\n",
        "        client=client,\n",
        "        text_column=text_column,\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    # 結果をDataFrameに追加\n",
        "    df['claude_decision'] = [r.judgement for r in results]\n",
        "    df['claude_reason'] = [r.reason for r in results]\n",
        "    df['claude_error'] = [r.error for r in results]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# テキストクリーニング関数\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # リスト記法が文字列に含まれている場合、それを除去\n",
        "        if text.startswith('[') and ']' in text:\n",
        "            text = text.replace('[', '', 1)\n",
        "            text = text.replace(']', '', 1)\n",
        "        \n",
        "        # 引用符を除去\n",
        "        text = text.replace('\"', '').replace(\"'\", \"\")\n",
        "        \n",
        "        # 特殊文字をチェック\n",
        "        import re\n",
        "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # 非ASCII文字を空白に置換\n",
        "        \n",
        "        return text.strip()\n",
        "    elif isinstance(text, list):\n",
        "        # リストの場合は各要素を文字列に変換して結合\n",
        "        return ' '.join([clean_text(str(item)) for item in text])\n",
        "    else:\n",
        "        # その他の型の場合は文字列に変換\n",
        "        return str(text)\n",
        "\n",
        "# データフレーム全体のテキスト列をクリーニング\n",
        "text_column = \"segments\"  # あなたのテキスト列名\n",
        "df_cleaned = df.copy()\n",
        "df_cleaned[text_column] = df_cleaned[text_column].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from typing import Optional, List\n",
        "from dataclasses import dataclass\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI  # OpenAI SDK を使用\n",
        "\n",
        "@dataclass\n",
        "class FullTextScreeningResult:\n",
        "    judgement: Optional[bool]  # 含める(True)/含めない(False)\n",
        "    reason: Optional[str]       # 理由\n",
        "    error: Optional[str] = None # APIエラーなどがあった場合\n",
        "\n",
        "\n",
        "class FullTextDecision(BaseModel):\n",
        "    judgement: bool\n",
        "    reason: str\n",
        "\n",
        "\n",
        "# テキストクリーニング関数\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # リスト記法が文字列に含まれている場合、それを除去\n",
        "        if text.startswith('[') and ']' in text:\n",
        "            text = text.replace('[', '', 1)\n",
        "            text = text.replace(']', '', 1)\n",
        "        \n",
        "        # 引用符を除去\n",
        "        text = text.replace('\"', '').replace(\"'\", \"\")\n",
        "        \n",
        "        # 特殊文字をチェック\n",
        "        import re\n",
        "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # 非ASCII文字を空白に置換\n",
        "        \n",
        "        return text.strip()\n",
        "    elif isinstance(text, list):\n",
        "        # リストの場合は各要素を文字列に変換して結合\n",
        "        return ' '.join([clean_text(str(item)) for item in text])\n",
        "    else:\n",
        "        # その他の型の場合は文字列に変換\n",
        "        return str(text)\n",
        "\n",
        "\n",
        "def get_fulltext_completion(\n",
        "    system_prompt: str,\n",
        "    prompt: str,\n",
        "    client: OpenAI,  # OpenAI クライアントを使用\n",
        "    model: str = \"anthropic/claude-3-7-sonnet\",  # OpenRouter のモデル名\n",
        "    temperature: float = 0.0,\n",
        "    timeout: int = 60  # タイムアウト設定（秒）\n",
        ") -> FullTextDecision:\n",
        "    \"\"\"\n",
        "    OpenRouter経由でClaudeにフルテキストを読ませ、含める／含めないの判断と理由を返してもらう関数。\n",
        "    \"\"\"\n",
        "    # クリーニング処理を追加\n",
        "    prompt = clean_text(prompt)\n",
        "    \n",
        "    # システムプロンプトでフォーマットの指示を行う\n",
        "    enhanced_system_prompt = f\"\"\"\n",
        "    {system_prompt}\n",
        "    \n",
        "    以下のテキストを評価し、指定された基準に基づいて含めるか除外するかを判断してください。\n",
        "    結果は以下のXMLフォーマットで必ず返してください:\n",
        "\n",
        "    <judgement>true または false</judgement>\n",
        "    <reason>判断の詳細な理由</reason>\n",
        "    \"\"\"\n",
        "\n",
        "    # OpenRouter APIを呼び出し (OpenAI の形式を使用)\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            temperature=temperature,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": enhanced_system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            extra_headers={\n",
        "                \"HTTP-Referer\": \"https://yourwebsite.com\",  # 必要に応じて変更\n",
        "                \"X-Title\": \"SystematicReviewApp\",          # 必要に応じて変更\n",
        "            },\n",
        "            timeout=timeout  # タイムアウト設定\n",
        "        )\n",
        "\n",
        "        # レスポンスからテキストを抽出\n",
        "        if hasattr(response, 'choices') and len(response.choices) > 0:\n",
        "            response_content = response.choices[0].message.content\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected response format: {response}\")\n",
        "\n",
        "        # XMLタグから判断と理由を抽出\n",
        "        judgement_start = response_content.find(\"<judgement>\") + len(\"<judgement>\")\n",
        "        judgement_end = response_content.find(\"</judgement>\")\n",
        "        judgement_text = response_content[judgement_start:judgement_end].strip().lower()\n",
        "        judgement = judgement_text == \"true\"\n",
        "\n",
        "        # 理由を抽出\n",
        "        reason_start = response_content.find(\"<reason>\") + len(\"<reason>\")\n",
        "        reason_end = response_content.find(\"</reason>\")\n",
        "        reason = response_content[reason_start:reason_end].strip()\n",
        "\n",
        "        return FullTextDecision(judgement=judgement, reason=reason)\n",
        "    except Exception as e:\n",
        "        # エラーが発生した場合はデバッグ情報を出力\n",
        "        print(f\"Error in API request: {str(e)}\")\n",
        "        \n",
        "        # エラーメッセージから判断を試みる\n",
        "        try:\n",
        "            if 'response_content' in locals():\n",
        "                # シンプルなヒューリスティックで判断を試みる\n",
        "                judgement = \"含める\" in response_content.lower() or \"include\" in response_content.lower()\n",
        "                return FullTextDecision(\n",
        "                    judgement=judgement,\n",
        "                    reason=\"APIレスポンスからの抽出に失敗しました。テキスト全体: \" + response_content[:200] + \"...\"\n",
        "                )\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # エラーを再度発生させる\n",
        "        raise\n",
        "\n",
        "\n",
        "def process_fulltext_abstracts(\n",
        "    df: pd.DataFrame,\n",
        "    system_prompt: str,\n",
        "    client: OpenAI,  # OpenAI クライアントを使用\n",
        "    text_column: str = \"segments\",  # フルテキストが入っている列名\n",
        "    max_retries: int = 3,\n",
        "    retry_delay: int = 5,\n",
        "    model: str = \"anthropic/claude-3-7-sonnet\",  # OpenRouter のモデル名\n",
        "    checkpoint_interval: int = 10,  # チェックポイントを保存する間隔（レコード数）\n",
        "    resume_from: int = 0  # 処理を再開するインデックス\n",
        ") -> List[FullTextScreeningResult]:\n",
        "    \"\"\"\n",
        "    DataFrameの各行に含まれるフルテキストをClaudeに投げて、\n",
        "    含める／含めない＆理由をまとめて返す。\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    total_records = len(df)\n",
        "    start_time = time.time()\n",
        "    checkpoint_filename = f\"screening_checkpoint_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    \n",
        "    # 進捗表示のためのカウンター\n",
        "    success_count = 0\n",
        "    error_count = 0\n",
        "    \n",
        "    # 進捗バーの初期化\n",
        "    print(f\"処理開始: 全{total_records}件 ({resume_from}件目から開始)\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # 結果を保存するための一時的なデータフレーム\n",
        "    temp_df = df.copy()\n",
        "    temp_df['claude_decision'] = None\n",
        "    temp_df['claude_reason'] = None\n",
        "    temp_df['claude_error'] = None\n",
        "\n",
        "    for i, (idx, row) in enumerate(df.iloc[resume_from:].iterrows(), start=resume_from):\n",
        "        text_to_screen = row.get(text_column, \"\")\n",
        "        current_result = None\n",
        "        error_message = None\n",
        "        \n",
        "        # 進捗を表示\n",
        "        elapsed_time = time.time() - start_time\n",
        "        if i > resume_from:\n",
        "            records_per_second = (i - resume_from) / elapsed_time if elapsed_time > 0 else 0\n",
        "            estimated_total_time = total_records / records_per_second if records_per_second > 0 else 0\n",
        "            estimated_remaining = estimated_total_time - elapsed_time if estimated_total_time > 0 else 0\n",
        "            \n",
        "            # 進捗率と残り時間を表示\n",
        "            progress_percent = (i / total_records) * 100\n",
        "            print(f\"\\r処理中: {i}/{total_records} ({progress_percent:.1f}%) | \"\n",
        "                  f\"成功: {success_count} | エラー: {error_count} | \"\n",
        "                  f\"残り約: {estimated_remaining/60:.1f}分\", end=\"\")\n",
        "\n",
        "        # テキストが欠損しているか非常に短い場合はスキップ\n",
        "        if pd.isna(text_to_screen) or (isinstance(text_to_screen, str) and len(text_to_screen.strip()) < 10):\n",
        "            print(f\"\\n[{i}] テキストが欠損または短すぎるためスキップします\")\n",
        "            current_result = FullTextScreeningResult(\n",
        "                judgement=None,\n",
        "                reason=\"テキストが欠損または短すぎるためスキップ\",\n",
        "                error=\"テキスト不足\"\n",
        "            )\n",
        "            results.append(current_result)\n",
        "            \n",
        "            # 一時的なデータフレームに結果を保存\n",
        "            temp_df.at[idx, 'claude_decision'] = current_result.judgement\n",
        "            temp_df.at[idx, 'claude_reason'] = current_result.reason\n",
        "            temp_df.at[idx, 'claude_error'] = current_result.error\n",
        "            \n",
        "            error_count += 1\n",
        "            continue\n",
        "\n",
        "        # テキストが長すぎる場合は警告\n",
        "        if isinstance(text_to_screen, str) and len(text_to_screen) > 15000:\n",
        "            print(f\"\\n[{i}] 警告: テキストが長すぎます ({len(text_to_screen)}文字)\")\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                print(f\"\\n[{i}] リクエスト開始: {time.strftime('%H:%M:%S')}\")\n",
        "                # Claudeに問い合わせ\n",
        "                decision = get_fulltext_completion(\n",
        "                    system_prompt=system_prompt,\n",
        "                    prompt=text_to_screen,\n",
        "                    client=client,\n",
        "                    model=model\n",
        "                )\n",
        "                print(f\"[{i}] リクエスト完了: {time.strftime('%H:%M:%S')}\")\n",
        "                \n",
        "                current_result = FullTextScreeningResult(\n",
        "                    judgement=decision.judgement,\n",
        "                    reason=decision.reason\n",
        "                )\n",
        "                \n",
        "                success_count += 1\n",
        "                break  # 成功したらリトライループを抜ける\n",
        "\n",
        "            except Exception as e:\n",
        "                error_message = str(e)\n",
        "                print(f\"\\n[{i}] Error (attempt {attempt+1}/{max_retries}): {error_message}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    print(\"Max retries reached. Moving to next.\")\n",
        "                    error_count += 1\n",
        "\n",
        "        # current_resultに成功結果が入っていなければ、エラーとして記録\n",
        "        if not current_result:\n",
        "            current_result = FullTextScreeningResult(\n",
        "                judgement=None,\n",
        "                reason=None,\n",
        "                error=error_message\n",
        "            )\n",
        "\n",
        "        results.append(current_result)\n",
        "        \n",
        "        # 一時的なデータフレームに結果を保存\n",
        "        temp_df.at[idx, 'claude_decision'] = current_result.judgement\n",
        "        temp_df.at[idx, 'claude_reason'] = current_result.reason\n",
        "        temp_df.at[idx, 'claude_error'] = current_result.error\n",
        "        \n",
        "        # チェックポイントを保存\n",
        "        if (i + 1) % checkpoint_interval == 0:\n",
        "            temp_df.to_csv(checkpoint_filename, index=False)\n",
        "            print(f\"\\nチェックポイント保存: {checkpoint_filename} ({i+1}/{total_records}件完了)\")\n",
        "\n",
        "    # 最終結果を保存\n",
        "    print(\"\\n\" + \"-\" * 50)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"処理完了: {total_records}件 (所要時間: {elapsed_time/60:.1f}分)\")\n",
        "    print(f\"成功: {success_count} | エラー: {error_count}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def process_fulltext_screening(\n",
        "    df: pd.DataFrame,\n",
        "    system_prompt: str,\n",
        "    client: OpenAI,  # OpenAI クライアントを使用\n",
        "    text_column: str = \"segments\",\n",
        "    model: str = \"anthropic/claude-3-7-sonnet\",  # OpenRouter のモデル名\n",
        "    resume_from: int = 0,  # 処理を再開するインデックス\n",
        "    checkpoint_interval: int = 10  # チェックポイント保存間隔\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    フルテキストスクリーニングを実行し、結果を元のDataFrameに追加して返す\n",
        "\n",
        "    Args:\n",
        "        df: 入力DataFrame\n",
        "        system_prompt: システムプロンプト\n",
        "        client: OpenAI APIクライアント\n",
        "        text_column: テキストが含まれる列名\n",
        "        model: 使用するモデル名\n",
        "        resume_from: 処理を再開するインデックス\n",
        "        checkpoint_interval: チェックポイント保存間隔\n",
        "\n",
        "    Returns:\n",
        "        結果を追加したDataFrame\n",
        "    \"\"\"\n",
        "    # 元のdfを変更しないようにコピー\n",
        "    df = df.copy()\n",
        "    \n",
        "    # 欠損値のチェック\n",
        "    missing_count = df[text_column].isna().sum()\n",
        "    if missing_count > 0:\n",
        "        print(f\"警告: {text_column}列に{missing_count}件の欠損値があります\")\n",
        "    \n",
        "    # 空文字列や短いテキストのチェック\n",
        "    empty_text_count = df[text_column].apply(lambda x: isinstance(x, str) and x.strip() == '').sum()\n",
        "    short_text_count = df[text_column].apply(lambda x: isinstance(x, str) and len(x.strip()) < 10).sum()\n",
        "    if empty_text_count > 0 or short_text_count > 0:\n",
        "        print(f\"警告: 空テキスト({empty_text_count}件)または短いテキスト({short_text_count}件)があります\")\n",
        "\n",
        "    # データクリーニングを適用\n",
        "    print(\"テキストクリーニングを適用中...\")\n",
        "    df[text_column] = df[text_column].apply(clean_text)\n",
        "    \n",
        "    # 各テキストに対してスクリーニングを実行\n",
        "    results = process_fulltext_abstracts(\n",
        "        df=df,\n",
        "        system_prompt=system_prompt,\n",
        "        client=client,\n",
        "        text_column=text_column,\n",
        "        model=model,\n",
        "        resume_from=resume_from,\n",
        "        checkpoint_interval=checkpoint_interval\n",
        "    )\n",
        "\n",
        "    # 結果をDataFrameに追加\n",
        "    df['claude_decision'] = [r.judgement for r in results]\n",
        "    df['claude_reason'] = [r.reason for r in results]\n",
        "    df['claude_error'] = [r.error for r in results]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# 列の存在確認と要約統計\n",
        "def validate_results(df):\n",
        "    # 1. 列が存在するか確認\n",
        "    expected_columns = ['folder_name', 'claude_decision', 'claude_reason', 'claude_error']\n",
        "    missing_columns = [col for col in expected_columns if col not in df.columns]\n",
        "    \n",
        "    if missing_columns:\n",
        "        print(f\"警告: 以下の列がデータフレームに存在しません: {missing_columns}\")\n",
        "    else:\n",
        "        print(\"✓ すべての予想される列が存在します\")\n",
        "    \n",
        "    # 2. 列の型と非NULLの数を確認\n",
        "    if 'claude_decision' in df.columns:\n",
        "        decision_counts = df['claude_decision'].value_counts(dropna=False)\n",
        "        print(\"\\n[claude_decision の集計]\")\n",
        "        print(f\"True の数: {decision_counts.get(True, 0)}\")\n",
        "        print(f\"False の数: {decision_counts.get(False, 0)}\")\n",
        "        print(f\"NULL の数: {decision_counts.get(None, 0)}\")\n",
        "        if len(df) > 0:\n",
        "            print(f\"判断の割合: {(decision_counts.get(True, 0) / len(df) * 100):.1f}% が含む\")\n",
        "    \n",
        "    # 3. reasonの長さの統計\n",
        "    if 'claude_reason' in df.columns:\n",
        "        # NULL以外のreasonの文字数\n",
        "        reason_lengths = df['claude_reason'].dropna().apply(len)\n",
        "        print(\"\\n[claude_reason の統計]\")\n",
        "        print(f\"理由の件数: {len(reason_lengths)} / {len(df)}\")\n",
        "        if not reason_lengths.empty:\n",
        "            print(f\"最短の理由: {reason_lengths.min()} 文字\")\n",
        "            print(f\"最長の理由: {reason_lengths.max()} 文字\")\n",
        "            print(f\"平均文字数: {reason_lengths.mean():.1f} 文字\")\n",
        "    \n",
        "    # 4. エラーの確認\n",
        "    if 'claude_error' in df.columns:\n",
        "        error_count = df['claude_error'].notna().sum()\n",
        "        print(f\"\\n[エラー]\")\n",
        "        print(f\"エラーの件数: {error_count} / {len(df)}\")\n",
        "        \n",
        "        if error_count > 0:\n",
        "            print(\"最初のエラー例:\")\n",
        "            first_error = df[df['claude_error'].notna()].iloc[0]\n",
        "            print(f\"  フォルダ名: {first_error['folder_name']}\")\n",
        "            print(f\"  エラー: {first_error['claude_error'][:100]}...\" if len(str(first_error['claude_error'])) > 100 else first_error['claude_error'])\n",
        "    \n",
        "    # 5. 処理状況の概要\n",
        "    total = len(df)\n",
        "    processed = df['claude_decision'].notna().sum()\n",
        "    print(f\"\\n[処理状況]\")\n",
        "    print(f\"処理完了: {processed} / {total} ({processed/total*100:.1f}%)\")\n",
        "    \n",
        "    return None  # 結果の内容は表示せず、統計情報のみ返す\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 使用例（実行部分）\n",
        "if __name__ == \"__main__\":\n",
        "    # APIキーを設定\n",
        "    OPENROUTER_API_KEY = \" \"  # 実際のキーに置き換えてください\n",
        "    \n",
        "    # OpenAI クライアントを初期化 (OpenRouter 用に設定)\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://openrouter.ai/api/v1\",\n",
        "        api_key=OPENROUTER_API_KEY\n",
        "    )\n",
        "    \n",
        "    # システムプロンプト例\n",
        "    system_prompt = \"\"\"\n",
        "    You are an expert in systematic reviews of clinical prediction models. Your role is to read full-text articles and decide whether each article should be included or excluded based on the criteria below. You must return a structured response containing:\n",
        "\n",
        "    A boolean \"judgement\" field:\n",
        "    - true if the article meets all inclusion criteria\n",
        "    - false if the article fails any criterion and should be excluded\n",
        "\n",
        "    A \"reason\" field:\n",
        "    - a concise explanation for why you decided to include or exclude the article\n",
        "\n",
        "    FULL-TEXT SCREENING CRITERIA\n",
        "\n",
        "    Inclusion Criteria:\n",
        "    - Study Focus: Diagnostic prediction models for bacteremia\n",
        "    - Population: Patients with suspected bacteremia who had blood cultures obtained\n",
        "    - Model Evaluation: Studies reporting model performance using AUC-ROC, C-statistics, or observed-to-expected events ratio (O:E ratio)\n",
        "    - Study Design: Any study design except those specified in exclusion criteria\n",
        "    - Language & Status: Any language, any publication status (including unpublished/conference abstracts)\n",
        "\n",
        "    Exclusion Criteria:\n",
        "    - Study Design: Two-gate studies (case-control studies where patients are recruited from different populations)\n",
        "    - Target Condition: Studies predicting bacteremia of specific microorganisms only (e.g., MRSA, ESBL-producing gram-negative bacteria)\n",
        "    - Model Complexity: Single predictor studies (models using only one variable such as CRP, procalcitonin, or specific interleukins)\n",
        "    - Target Condition: Studies where the primary target condition is not bacteremia\n",
        "\n",
        "    Additional Notes:\n",
        "    - Studies developing or validating multivariable prediction models should be included\n",
        "    - No restrictions on publication year, geographical location, or clinical setting\n",
        "    - External validation studies are eligible if they evaluate a prediction model for bacteremia\n",
        "\n",
        "    IMPORTANT INSTRUCTIONS FOR THE OUTPUT:\n",
        "    - judgement must be returned strictly as either true or false\n",
        "    - reason must always be included, referencing specific criteria\n",
        "    - Never provide extra fields in the final structured response\n",
        "\n",
        "    Provide your response using the following XML format:\n",
        "    <judgement>true or false</judgement>\n",
        "    <reason>Your detailed justification here</reason>\n",
        "    \"\"\"\n",
        "    \n",
        "    # ここでデータフレームを読み込む（実際のコードに合わせてください）\n",
        "    # df = pd.read_csv('your_data.csv')  # 実際のデータファイルパスに置き換え\n",
        "    \n",
        "    # 再開するインデックスを指定（0から始める場合は0、途中から再開する場合は該当するインデックス）\n",
        "    resume_from = 0  # 例: 134から再開する場合は134を指定\n",
        "    \n",
        "    # フルテキストスクリーニングを実行\n",
        "    result_df = process_fulltext_screening(\n",
        "        df=df_cleaned,  # 実際のデータフレームを使用\n",
        "        system_prompt=system_prompt,\n",
        "        client=client,\n",
        "        model=\"anthropic/claude-3-7-sonnet\",  # OpenRouter で使用可能な Claude 3.7 Sonnet\n",
        "        resume_from=resume_from,\n",
        "        checkpoint_interval=10  # 10件ごとにチェックポイントを保存\n",
        "    )\n",
        "    \n",
        "    # 結果を確認（直接内容を見ずに統計情報を確認）\n",
        "    validate_results(result_df)\n",
        "    \n",
        "    # 結果を CSV に保存\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    result_filename = f'screening_results_{timestamp}.csv'\n",
        "    result_df.to_csv(result_filename, index=False)\n",
        "    print(f\"\\n結果を '{result_filename}' に保存しました\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 元のCSVファイルを読み込む\n",
        "file_path = 'full_text_without_tab_and_fig_output.csv'  # 実際のファイルパスに置き換え\n",
        "original_df = pd.read_csv(file_path)\n",
        "\n",
        "# 列名を確認\n",
        "print(\"元のCSVファイルの列名:\")\n",
        "print(original_df.columns.tolist())\n",
        "print(f\"行数: {len(original_df)}\")\n",
        "\n",
        "print(\"\\nresult_dfの列名:\")\n",
        "print(result_df.columns.tolist())\n",
        "print(f\"行数: {len(result_df)}\")\n",
        "\n",
        "# マージのために元のデータフレームのキー列を確認\n",
        "print(\"\\npdf_key列の最初の5件:\")\n",
        "if 'pdf_key' in original_df.columns:\n",
        "    print(original_df['pdf_key'].head().tolist())\n",
        "else:\n",
        "    print(\"pdf_key列が存在しません\")\n",
        "\n",
        "print(\"\\npdf_filename列の最初の5件:\")\n",
        "if 'pdf_filename' in original_df.columns:\n",
        "    print(original_df['pdf_filename'].head().tolist())\n",
        "else:\n",
        "    print(\"pdf_filename列が存在しません\")\n",
        "\n",
        "print(\"\\nfolder_name列の最初の5件:\")\n",
        "print(result_df['folder_name'].head().tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# マージ方法の決定\n",
        "# folder_nameがpdf_keyと一致するかpdf_filenameと一致するかを確認\n",
        "match_with_key = False\n",
        "match_with_filename = False\n",
        "\n",
        "# サンプルデータでマッチングを試行\n",
        "sample_folder_names = result_df['folder_name'].head(5).tolist()\n",
        "\n",
        "if 'pdf_key' in original_df.columns:\n",
        "    sample_pdf_keys = original_df['pdf_key'].head(20).astype(str).tolist()\n",
        "    for name in sample_folder_names:\n",
        "        if name in sample_pdf_keys:\n",
        "            match_with_key = True\n",
        "            print(f\"folder_name '{name}' は pdf_key と一致します\")\n",
        "            break\n",
        "\n",
        "if 'pdf_filename' in original_df.columns:\n",
        "    sample_pdf_filenames = original_df['pdf_filename'].head(20).astype(str).tolist()\n",
        "    for name in sample_folder_names:\n",
        "        if name in sample_pdf_filenames:\n",
        "            match_with_filename = True\n",
        "            print(f\"folder_name '{name}' は pdf_filename と一致します\")\n",
        "            break\n",
        "\n",
        "# マージのためのキー列を選択\n",
        "if match_with_key:\n",
        "    print(\"\\npdf_key列とfolder_name列でマージを実行します\")\n",
        "    merge_key_original = 'pdf_key'\n",
        "elif match_with_filename:\n",
        "    print(\"\\npdf_filename列とfolder_name列でマージを実行します\")\n",
        "    merge_key_original = 'pdf_filename'\n",
        "else:\n",
        "    print(\"\\n自動マッチングできませんでした。pdf_key列を使用してマージを試みます\")\n",
        "    merge_key_original = 'pdf_key'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# マージを実行\n",
        "# まず型を一致させる\n",
        "original_df[merge_key_original] = original_df[merge_key_original].astype(str)\n",
        "result_df['folder_name'] = result_df['folder_name'].astype(str)\n",
        "\n",
        "# マージの実行\n",
        "merged_df = pd.merge(\n",
        "    original_df, \n",
        "    result_df, \n",
        "    left_on=merge_key_original, \n",
        "    right_on='folder_name', \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# マージ結果の確認\n",
        "print(f\"\\nマージ後のデータフレームの行数: {len(merged_df)}\")\n",
        "print(f\"列数: {len(merged_df.columns)}\")\n",
        "print(f\"claude_decision列のNull値の数: {merged_df['claude_decision'].isna().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# タイムスタンプを生成\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSVとして保存\n",
        "csv_filename = f'merged_results_{timestamp}.csv'\n",
        "merged_df.to_csv(csv_filename, index=False)\n",
        "print(f\"\\nCSVファイルとして保存しました: {csv_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Excelとして保存\n",
        "excel_filename = f'merged_results_{timestamp}.xlsx'\n",
        "merged_df.to_excel(excel_filename, index=False)\n",
        "print(f\"Excelファイルとして保存しました: {excel_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 保存したファイルのフルパスを表示\n",
        "current_dir = os.getcwd()\n",
        "print(f\"\\nCSVファイルのフルパス: {os.path.join(current_dir, csv_filename)}\")\n",
        "print(f\"Excelファイルのフルパス: {os.path.join(current_dir, excel_filename)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result_dfの列名を表示\n",
        "print(\"result_dfの列名:\")\n",
        "print(result_df.columns.tolist())\n",
        "\n",
        "# 列数も表示\n",
        "print(f\"列数: {len(result_df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "T6n_JvUb2siL",
        "outputId": "2408e7df-991e-4ca8-9ac1-84d7b1408e0e"
      },
      "outputs": [],
      "source": [
        "# 使用例\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    # システムプロンプト\n",
        "    system_prompt = \"\"\"\n",
        "You are an expert in systematic reviews of clinical prediction models. Your role is to read full-text articles and decide whether each article should be included or excluded based on the criteria below. You must return a structured response containing:\n",
        "\n",
        "A boolean \"judgement\" field:\n",
        "- true if the article meets all inclusion criteria\n",
        "- false if the article fails any criterion and should be excluded\n",
        "\n",
        "A \"reason\" field:\n",
        "- a concise explanation for why you decided to include or exclude the article\n",
        "\n",
        "FULL-TEXT SCREENING CRITERIA\n",
        "\n",
        "Inclusion Criteria:\n",
        "- Study Focus: Diagnostic prediction models for bacteremia\n",
        "- Population: Patients with suspected bacteremia who had blood cultures obtained\n",
        "- Model Evaluation: Studies reporting model performance using AUC-ROC, C-statistics, or observed-to-expected events ratio (O:E ratio)\n",
        "- Study Design: Any study design except those specified in exclusion criteria\n",
        "- Language & Status: Any language, any publication status (including unpublished/conference abstracts)\n",
        "\n",
        "Exclusion Criteria:\n",
        "- Study Design: Two-gate studies (case-control studies where patients are recruited from different populations)\n",
        "- Target Condition: Studies predicting bacteremia of specific microorganisms only (e.g., MRSA, ESBL-producing gram-negative bacteria)\n",
        "- Model Complexity: Single predictor studies (models using only one variable such as CRP, procalcitonin, or specific interleukins)\n",
        "- Target Condition: Studies where the primary target condition is not bacteremia\n",
        "\n",
        "Additional Notes:\n",
        "- Studies developing or validating multivariable prediction models should be included\n",
        "- No restrictions on publication year, geographical location, or clinical setting\n",
        "- External validation studies are eligible if they evaluate a prediction model for bacteremia\n",
        "\n",
        "IMPORTANT INSTRUCTIONS FOR THE OUTPUT:\n",
        "- judgement must be returned strictly as either true or false\n",
        "- reason must always be included, referencing specific criteria\n",
        "- Never provide extra fields in the final structured response\n",
        "\n",
        "Provide your response using the following XML format:\n",
        "<judgement>true or false</judgement>\n",
        "<reason>Your detailed justification here</reason>\n",
        "    \"\"\"\n",
        "\n",
        "    # 処理実行\n",
        "    df = process_fulltext_screening(\n",
        "        df=df,\n",
        "        system_prompt=system_prompt,\n",
        "        client=client,\n",
        "        text_column=\"segments\",\n",
        "        model=\"gpt-4o\"\n",
        "    )\n",
        "\n",
        "    # 結果を保存\n",
        "    df.to_csv(\"fulltext_screened_results_gpt.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya8UTcgw2wzK"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAnXCUOR20V6"
      },
      "outputs": [],
      "source": [
        "#segments列を削除\n",
        "df_answer = df.drop('segments', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7B3dZJG23kW"
      },
      "outputs": [],
      "source": [
        "df_answer.to_excel(\"fulltext_screened_results_gpt.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL5Jm0R8boI3"
      },
      "outputs": [],
      "source": [
        "#文献データを格納した、スクリーニング用のスプレッドシートを開く\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Google Colabで認証\n",
        "auth.authenticate_user()\n",
        "\n",
        "# スプレッドシートをインポート（自分のスプレッドシートIDに置き換える）\n",
        "SPREADSHEET_ID = 'your_spreadsheet_id_here'  # スプレッドシートIDを指定\n",
        "sheet_name =   \" \"  # シート名を指定\n",
        "\n",
        "# Google Sheetsから直接データを読み込む\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# スプレッドシートを開く\n",
        "worksheet = gc.open_by_key(SPREADSHEET_ID).worksheet(sheet_name)\n",
        "data = worksheet.get_all_values()\n",
        "headers = data[0]\n",
        "df = pd.DataFrame(data[1:], columns=headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install rispy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install gdown requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "import re\n",
        "\n",
        "# ダウンロード先のフォルダを作成\n",
        "output_dir = \"downloaded_files\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Google DriveリンクのリストからIDを抽出する関数\n",
        "def extract_file_id(url):\n",
        "    # Google DriveリンクからIDを抽出\n",
        "    if 'drive.google.com' in url:\n",
        "        match = re.search(r'/d/([a-zA-Z0-9_-]+)', url)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    # PMCリンクの場合はそのままURLを返す\n",
        "    elif 'pmc.ncbi.nlm.nih.gov' in url:\n",
        "        return url\n",
        "    return None\n",
        "\n",
        "# リンクのリスト（テキストファイルから読み込むこともできます）\n",
        "urls = \"\"\n",
        "\n",
        "# 改行で分割してリストに変換\n",
        "url_list = [url.strip() for url in urls.strip().split(\"\\n\") if url.strip()]\n",
        "\n",
        "# 各URLを処理\n",
        "for i, url in enumerate(url_list):\n",
        "    file_id = extract_file_id(url)\n",
        "    \n",
        "    if file_id:\n",
        "        if 'pmc.ncbi.nlm.nih.gov' in url:\n",
        "            # PMCリンクの場合は標準的なダウンロード方法を使用\n",
        "            import requests\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                if response.status_code == 200:\n",
        "                    filename = os.path.join(output_dir, f\"pmc_file_{i+1}.pdf\")\n",
        "                    with open(filename, 'wb') as f:\n",
        "                        f.write(response.content)\n",
        "                    print(f\"ダウンロード成功: {filename}\")\n",
        "                else:\n",
        "                    print(f\"ダウンロード失敗: {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"エラー: {url} - {str(e)}\")\n",
        "        else:\n",
        "            # Google Driveの場合はgdownを使用\n",
        "            try:\n",
        "                output_file = os.path.join(output_dir, f\"file_{i+1}.pdf\")\n",
        "                gdown.download(id=file_id, output=output_file, quiet=False)\n",
        "                print(f\"ダウンロード成功: {output_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"エラー: {url} - {str(e)}\")\n",
        "    else:\n",
        "        print(f\"無効なURL: {url}\")\n",
        "\n",
        "print(\"ダウンロード完了！\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
